{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c97fd8af7e49418190e89b968c36d3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30391edb9dc046aaabf11a1acced6633",
              "IPY_MODEL_7a9ef17d4d5a4195a39eafba79ecb5d2",
              "IPY_MODEL_e58146bf0fbb4164a3a677fe354dddc7"
            ],
            "layout": "IPY_MODEL_32ede6f55519440d8f2f0749584ff0be"
          }
        },
        "30391edb9dc046aaabf11a1acced6633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_747b60403db94b44b88e233320be4f77",
            "placeholder": "​",
            "style": "IPY_MODEL_df7c75874a0c4533b349c7070a1f080e",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "7a9ef17d4d5a4195a39eafba79ecb5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fe3f26b78614837bb378a8954be0fa3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5d296b3c54e41ff88e01d04f7b4a591",
            "value": 2
          }
        },
        "e58146bf0fbb4164a3a677fe354dddc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61d10d2b3ae4cf9a0e6c6772a7932b5",
            "placeholder": "​",
            "style": "IPY_MODEL_10889d7471be4ce097a22d912a939260",
            "value": " 2/2 [00:01&lt;00:00,  1.96it/s]"
          }
        },
        "32ede6f55519440d8f2f0749584ff0be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "747b60403db94b44b88e233320be4f77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df7c75874a0c4533b349c7070a1f080e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fe3f26b78614837bb378a8954be0fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d296b3c54e41ff88e01d04f7b4a591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e61d10d2b3ae4cf9a0e6c6772a7932b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10889d7471be4ce097a22d912a939260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d08519757a134b888b1708c9dc664d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_088497c1bcf2431caa748c8a5ff4fb80",
              "IPY_MODEL_cad77d9228a64539bc66788b907b9f1f",
              "IPY_MODEL_29ba00c1748841f2b6d703482d2db8ca"
            ],
            "layout": "IPY_MODEL_a3203a43bb394e53b44a573c4f9d1554"
          }
        },
        "088497c1bcf2431caa748c8a5ff4fb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ad3d6411ce547909d5f6f8ee4b8d705",
            "placeholder": "​",
            "style": "IPY_MODEL_5abc109dedfe489fa7f963b45648aeb7",
            "value": "Epoch 0:  16%"
          }
        },
        "cad77d9228a64539bc66788b907b9f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab60a4af7f1848b99feb3d5c11d3a68c",
            "max": 47110,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62976af232e14041997b6a75872cb677",
            "value": 7500
          }
        },
        "29ba00c1748841f2b6d703482d2db8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd59ebfb16d4963bd17fcd3c7d49abe",
            "placeholder": "​",
            "style": "IPY_MODEL_6081337b6ad546d3b9bf9124d371b991",
            "value": " 7500/47110 [33:53&lt;2:59:01,  3.69it/s, loss=0.679, v_num=2, train_loss_step=0.657]"
          }
        },
        "a3203a43bb394e53b44a573c4f9d1554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0ad3d6411ce547909d5f6f8ee4b8d705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5abc109dedfe489fa7f963b45648aeb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab60a4af7f1848b99feb3d5c11d3a68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62976af232e14041997b6a75872cb677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cd59ebfb16d4963bd17fcd3c7d49abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6081337b6ad546d3b9bf9124d371b991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9b2fa9cf2874620ae76b1d533cc6f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cabcdcabfd82419ba7a5c11a8db435ab",
              "IPY_MODEL_7a5393e2277947feaaa5f53bc501a82c",
              "IPY_MODEL_25fb20ed6b8a41248d6f68f369233b6e"
            ],
            "layout": "IPY_MODEL_ccd588a4e2ab46e182d7b64e8f0802b2"
          }
        },
        "cabcdcabfd82419ba7a5c11a8db435ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcf76247b5a24fc9a844d51ee2890277",
            "placeholder": "​",
            "style": "IPY_MODEL_99cd371ce4ee4efb98dd730782155525",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "7a5393e2277947feaaa5f53bc501a82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7c028a11494dabb457560ded31f84d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffb4fb847b5e48a9a2858c3a4deb7c2a",
            "value": 2
          }
        },
        "25fb20ed6b8a41248d6f68f369233b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd3acd079cc4f959886e05cd6516029",
            "placeholder": "​",
            "style": "IPY_MODEL_65b9daf0ed0d4be88bf04354864dc3f8",
            "value": " 2/2 [00:01&lt;00:00,  1.19it/s]"
          }
        },
        "ccd588a4e2ab46e182d7b64e8f0802b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "dcf76247b5a24fc9a844d51ee2890277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99cd371ce4ee4efb98dd730782155525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e7c028a11494dabb457560ded31f84d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffb4fb847b5e48a9a2858c3a4deb7c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bd3acd079cc4f959886e05cd6516029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b9daf0ed0d4be88bf04354864dc3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6831056493ac42438957796550288ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9d1e76ecd4a4105b1c52cce361afa43",
              "IPY_MODEL_07c6e71e82754757ab5c883548e1b245",
              "IPY_MODEL_f284487f9aea4c56bffd87ac8ba59316"
            ],
            "layout": "IPY_MODEL_b1c6524a347f4b42b002ca5e65ab0e8f"
          }
        },
        "b9d1e76ecd4a4105b1c52cce361afa43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f17aa809f3941fe820451453dad9638",
            "placeholder": "​",
            "style": "IPY_MODEL_1da508a84f0d4530a28d981257e0ee39",
            "value": "Epoch 0:   0%"
          }
        },
        "07c6e71e82754757ab5c883548e1b245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2b71dc7c2d94c66b081336ebbf51b36",
            "max": 23438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a625391db284596a1acc34c8438cc17",
            "value": 20
          }
        },
        "f284487f9aea4c56bffd87ac8ba59316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8ca3c874b724e99ac1309359ae50383",
            "placeholder": "​",
            "style": "IPY_MODEL_31e66d237d93492980c81e6c93e5b7e3",
            "value": " 20/23438 [00:07&lt;2:31:27,  2.58it/s, v_num=0, train_loss_step=1.490]"
          }
        },
        "b1c6524a347f4b42b002ca5e65ab0e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7f17aa809f3941fe820451453dad9638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1da508a84f0d4530a28d981257e0ee39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2b71dc7c2d94c66b081336ebbf51b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a625391db284596a1acc34c8438cc17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8ca3c874b724e99ac1309359ae50383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e66d237d93492980c81e6c93e5b7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "747b77f860224de39a44cdb646710024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4283f9d98e2543ed9e9eaf393604da9d",
              "IPY_MODEL_2aa026eeb61b430e93e75fb9f63f12ad",
              "IPY_MODEL_f62eca98f001462bbd594293be6c51ca"
            ],
            "layout": "IPY_MODEL_82e94c71469346e9af6a739503377d85"
          }
        },
        "4283f9d98e2543ed9e9eaf393604da9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c890c18837f40748482ac79f472e842",
            "placeholder": "​",
            "style": "IPY_MODEL_35d2dafc5d5d4d4eade08b9674ad668c",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "2aa026eeb61b430e93e75fb9f63f12ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1bfccfd8002495c9011e71a8cf51d2f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66243573953f4b43bc05914ff693319f",
            "value": 2
          }
        },
        "f62eca98f001462bbd594293be6c51ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66ead7e7cdc44418a8873e6f42d35a8",
            "placeholder": "​",
            "style": "IPY_MODEL_fa1b4910f2d843baabb3af8bab2392a8",
            "value": " 2/2 [00:00&lt;00:00,  4.77it/s]"
          }
        },
        "82e94c71469346e9af6a739503377d85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "5c890c18837f40748482ac79f472e842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d2dafc5d5d4d4eade08b9674ad668c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1bfccfd8002495c9011e71a8cf51d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66243573953f4b43bc05914ff693319f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f66ead7e7cdc44418a8873e6f42d35a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa1b4910f2d843baabb3af8bab2392a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fced898423e4ca290680749c2559703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b98d42b76b8f46bb9c690b170b29c960",
              "IPY_MODEL_0fd9b66b9e0a463ea61fef3094988f32",
              "IPY_MODEL_e525ed1f163c476e80ab09c4f8d2f230"
            ],
            "layout": "IPY_MODEL_065bb6fe03414729912bd938831175d2"
          }
        },
        "b98d42b76b8f46bb9c690b170b29c960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de1bceac10f74089aa21c468d6fb6cbc",
            "placeholder": "​",
            "style": "IPY_MODEL_05b26fe71b56429bb89e91244b0f6329",
            "value": "Epoch 0:   3%"
          }
        },
        "0fd9b66b9e0a463ea61fef3094988f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe092da7f4a04a67a0bae3c71a6ac3c8",
            "max": 46875,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75fee4e1d92842c383ef7769abb7fc1d",
            "value": 1280
          }
        },
        "e525ed1f163c476e80ab09c4f8d2f230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f572dcb54fc54c3eac4a2c047c0a6255",
            "placeholder": "​",
            "style": "IPY_MODEL_36dead6f206c470fb8028ed8e3f53623",
            "value": " 1280/46875 [01:50&lt;1:05:48, 11.55it/s, v_num=3, train_loss_step=1.210]"
          }
        },
        "065bb6fe03414729912bd938831175d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "de1bceac10f74089aa21c468d6fb6cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b26fe71b56429bb89e91244b0f6329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe092da7f4a04a67a0bae3c71a6ac3c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75fee4e1d92842c383ef7769abb7fc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f572dcb54fc54c3eac4a2c047c0a6255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36dead6f206c470fb8028ed8e3f53623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiantonelli/DeepLearning-Project/blob/main/Deep_Learning_Project_Antonelli_Cuconasu_Gaudenzi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports"
      ],
      "metadata": {
        "id": "_0HXoR9RpcO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fxGf0hOgnsPD",
        "outputId": "799c0016-b31c-4950-c8ad-0ea23073c403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m715.6/715.6 KB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install torchmetrics --quiet\n",
        "!pip install gdown==4.5.4 --no-cache-dir --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import pickle\n",
        "from typing import *\n",
        "from datetime import datetime\n",
        "\n",
        "import gdown\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "import random\n",
        "\n",
        "# For reproducibility\n",
        "seed_everything(10, workers=True)"
      ],
      "metadata": {
        "id": "DobcczcWqek-",
        "outputId": "0354da04-f2e1-46d6-adfc-585ebdac1b4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url = \"https://drive.google.com/drive/folders/1LrGmpT6nVvcWOk-gy656xlFqmH8fIY7k?usp=sharing\"\n",
        "# gdown.download_folder(url=url, quiet=True, use_cookies=False, remaining_ok=True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVNGXDrHt71Y",
        "outputId": "effda797-3bfc-4139-97a3-577d40b1dee5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset_folder_path = \"/content/drive/MyDrive/Colab Notebooks/Deep Learning/DeepLearningProject-Shared\"\n",
        "dataset_folder_path = \"/content/drive/MyDrive/Deep_Learning_Project\"\n",
        "# dataset_folder_path = \"/content/DeepLearning-Shared\"\n",
        "os.chdir(dataset_folder_path)"
      ],
      "metadata": {
        "id": "gDIk7qc1uP4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89bxAqHuiCnI",
        "outputId": "f197ea32-cf42-4d7b-bd1d-9b2701463d11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " datasets\t\t   'Training_LSTM&GRU.gdoc'\n",
            " mathematics_dataset-v1.0   Training_TP_Transformer.gdoc\n",
            " modules.txt\t\t    Training_Transformer_Vanilla.gdoc\n",
            " training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ],
      "metadata": {
        "id": "PKSCee77ltVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we analyzed all the dataset files to retrieve the characters that will compose the vocabulary. Indeed, we wanted to be sure that our vocabulary contains all the files characters regardless the module we are working on.\n",
        "\n",
        "Moreover, after this pre-processing phase we decided to add the special token `<unk>` (i.e., unknown). Thus, if during inference we are using characters that are not in the vocabulary, we are still able to pre-processes the input, since whathever unknown character is replaced by that special token.  "
      ],
      "metadata": {
        "id": "dN7stnivltVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(text_path: str, lowercase: bool=True) -> Tuple[List[str], List[str]]:\n",
        "    questions = []\n",
        "    answers = []\n",
        "    with open(text_path, 'r') as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            row = line.rstrip().lower() if lowercase else line.rstrip()\n",
        "            # Questions\n",
        "            if idx % 2 == 0:\n",
        "                questions.append(row) \n",
        "            # Answers\n",
        "            else: \n",
        "                answers.append(row)\n",
        "    return questions, answers"
      ],
      "metadata": {
        "id": "BhJnHS2yltVl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(lists_of_texts: List[List[str]]) -> Set[str]:\n",
        "    unified_text = []\n",
        "    \n",
        "    for l in lists_of_texts:\n",
        "        unified_text += l\n",
        "\n",
        "    return Counter(\" \".join(unified_text)).keys()"
      ],
      "metadata": {
        "id": "ttex8hmLltVl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all files\n",
        "folders = ['extrapolate', 'interpolate', 'train-easy', 'train-medium', 'train-hard']\n",
        "files = []\n",
        "\n",
        "for fold in folders:\n",
        "    files += glob.glob(f\"./mathematics_dataset-v1.0/{fold}/*.txt\")"
      ],
      "metadata": {
        "id": "C9VHWlDQltVm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5684217d-7e3b-465e-9b1d-a1632b8feb7f",
        "id": "clPtTD2gltVm"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./mathematics_dataset-v1.0/extrapolate/arithmetic__add_sub_multiple_longer.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/algebra__polynomial_roots_big.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/arithmetic__add_or_sub_big.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/arithmetic__div_big.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/arithmetic__mul_div_multiple_longer.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files_vocabulary(files: List[str], save: bool=False) -> List[str]:\n",
        "    vocabulary = {}\n",
        "    all_lists = []\n",
        "\n",
        "    i = 0\n",
        "    for f in files:\n",
        "        train, test = read_dataset(f)\n",
        "        all_lists += train\n",
        "        all_lists += test\n",
        "        \n",
        "        # Set union\n",
        "        vocabulary |= get_vocabulary(all_lists)\n",
        "        all_lists = []\n",
        "\n",
        "        # Save the vocabulary up to now\n",
        "        if save and i % 10 == 0:\n",
        "            vocabulary = sorted(list(vocabulary))\n",
        "            with open('./datasets/pre_vocabulary.pkl', 'wb') as f:\n",
        "                pickle.dump(vocabulary, f)\n",
        "\n",
        "    # Save sorted vocabulary\n",
        "    vocabulary = sorted(list(vocabulary))\n",
        "    with open('./datasets/pre_vocabulary.pkl', 'wb') as f:\n",
        "        pickle.dump(vocabulary, f)\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "-NIjuy8IltVn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This operation requires quite a bit of time (~ 25 min), as we are scanning all the files. So, it is commented to avoid executing it.\n",
        "\n",
        "    vocabulary = get_files_vocabulary(files)"
      ],
      "metadata": {
        "id": "59SVX8yXltVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary_from_set(voc):\n",
        "    vocabulary = {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "    i = 4\n",
        "    for v in voc:\n",
        "        vocabulary[v] = i\n",
        "        i += 1\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "XnpfxJ_EltVn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./datasets/vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)"
      ],
      "metadata": {
        "id": "IA8mFT9WltVn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc2d872-5794-416f-e9d7-89aec731e358",
        "id": "1MoR8XGcltVp"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = create_vocabulary_from_set(vocabulary)"
      ],
      "metadata": {
        "id": "rSZUg9bCltVp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f79ea7-f98e-42c1-a398-e06a11743812",
        "id": "jUSSoxYsltVp"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<bos>': 1,\n",
              " '<eos>': 2,\n",
              " '<unk>': 3,\n",
              " ' ': 4,\n",
              " '!': 5,\n",
              " \"'\": 6,\n",
              " '(': 7,\n",
              " ')': 8,\n",
              " '*': 9,\n",
              " '+': 10,\n",
              " ',': 11,\n",
              " '-': 12,\n",
              " '.': 13,\n",
              " '/': 14,\n",
              " '0': 15,\n",
              " '1': 16,\n",
              " '2': 17,\n",
              " '3': 18,\n",
              " '4': 19,\n",
              " '5': 20,\n",
              " '6': 21,\n",
              " '7': 22,\n",
              " '8': 23,\n",
              " '9': 24,\n",
              " ':': 25,\n",
              " '<': 26,\n",
              " '=': 27,\n",
              " '>': 28,\n",
              " '?': 29,\n",
              " 'a': 30,\n",
              " 'b': 31,\n",
              " 'c': 32,\n",
              " 'd': 33,\n",
              " 'e': 34,\n",
              " 'f': 35,\n",
              " 'g': 36,\n",
              " 'h': 37,\n",
              " 'i': 38,\n",
              " 'j': 39,\n",
              " 'k': 40,\n",
              " 'l': 41,\n",
              " 'm': 42,\n",
              " 'n': 43,\n",
              " 'o': 44,\n",
              " 'p': 45,\n",
              " 'q': 46,\n",
              " 'r': 47,\n",
              " 's': 48,\n",
              " 't': 49,\n",
              " 'u': 50,\n",
              " 'v': 51,\n",
              " 'w': 52,\n",
              " 'x': 53,\n",
              " 'y': 54,\n",
              " 'z': 55,\n",
              " '{': 56,\n",
              " '}': 57}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "jSp1SqYRiYkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_module_paths(modules: List[str], difficulty: List[str]) -> List[str]:\n",
        "    paths = []\n",
        "\n",
        "    folders = ['train-easy', 'train-medium', 'train-hard']\n",
        "    \n",
        "    if difficulty is not None and set(difficulty).issubset(set(folders)):\n",
        "        folders = difficulty\n",
        "\n",
        "    for module in modules:\n",
        "        for fold in folders:\n",
        "            paths += glob.glob(f\"./mathematics_dataset-v1.0/{fold}/{module}.txt\")\n",
        "\n",
        "    return paths"
      ],
      "metadata": {
        "id": "mM60pprQiaQ-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module_files = get_train_module_paths([\"algebra__linear_1d\", \"algebra__linear_2d\"], difficulty=['train-easy'])\n",
        "module_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMYdwf5PibSx",
        "outputId": "a901457f-f376-42e3-b98a-ab0e93b06acb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt',\n",
              " './mathematics_dataset-v1.0/train-easy/algebra__linear_2d.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_module_paths(modules: List[str]) -> List[str]:\n",
        "    paths = []\n",
        "\n",
        "    for module in modules:\n",
        "        paths += glob.glob(f\"./mathematics_dataset-v1.0/interpolate/{module}.txt\")\n",
        "\n",
        "    return paths"
      ],
      "metadata": {
        "id": "Jxr8UcdfidO6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# algebra_train, algebra_test = read_all_module_files(\"algebra__linear_1d\")"
      ],
      "metadata": {
        "id": "M3RGCojMidzD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(algebra_train)"
      ],
      "metadata": {
        "id": "pgi-i53Piezk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algebra_path = \"./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt\"\n",
        "probability_path = \"./mathematics_dataset-v1.0/train-easy/probability__swr_p_level_set.txt\"\n",
        "prime_path = \"./mathematics_dataset-v1.0/train-easy/numbers__is_prime.txt\""
      ],
      "metadata": {
        "id": "9YnrTBHsigmg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "questions_easy_algebra, answers_easy_algebra = read_dataset(algebra_path)\n",
        "questions_easy_probability, answers_easy_probability = read_dataset(probability_path)\n",
        "questions_easy_prime, answers_easy_prime = read_dataset(prime_path)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EzvNILPFigiI",
        "outputId": "105b86ef-5ef0-427b-915e-d46145685980"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nquestions_easy_algebra, answers_easy_algebra = read_dataset(algebra_path)\\nquestions_easy_probability, answers_easy_probability = read_dataset(probability_path)\\nquestions_easy_prime, answers_easy_prime = read_dataset(prime_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no substantial difference in time between loading the entire dataset and pre-processing the data in the __getitem__ method:\n",
        "\n",
        "40 - 60 microsec vs 200 - 300 microsec"
      ],
      "metadata": {
        "id": "WLUVwIiKPqz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d = Mathematics_Dataset(module_files, v)"
      ],
      "metadata": {
        "id": "TK_H6cn3KOED"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# d[8]"
      ],
      "metadata": {
        "id": "0a0xRSGrKQXG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mathematics_Dataset(Dataset):\n",
        "    def __init__(self, modules_paths: List[str], vocabulary: Dict[str, int], max_len_question: int=160, max_len_answer: int=30):\n",
        "        super().__init__()\n",
        "        self.modules_paths = modules_paths\n",
        "        \n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        \n",
        "        for m in self.modules_paths:\n",
        "            q_m, a_m = self.read_dataset(m)\n",
        "            self.questions += q_m\n",
        "            self.answers += a_m\n",
        "        \n",
        "        self.max_len_question = max_len_question\n",
        "        self.max_len_answer = max_len_answer\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "    def read_dataset(self, text_path: str, lowercase: bool=True) -> Tuple[List[str], List[str]]:\n",
        "        questions = []\n",
        "        answers = []\n",
        "        with open(text_path, 'r') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                row = line.rstrip().lower() if lowercase else line.rstrip()\n",
        "                # Questions\n",
        "                if idx % 2 == 0:\n",
        "                    questions.append(row) \n",
        "                # Answers\n",
        "                else: \n",
        "                    answers.append(row)\n",
        "        return questions, answers\n",
        "\n",
        "    def convert_chars_to_ids(self, sentence: str, max_len: int) -> torch.tensor:\n",
        "        sentence_ids = np.full(max_len + 2, self.vocabulary['<pad>'])\n",
        "\n",
        "        # Start with <bos>\n",
        "        sentence_ids[0] = self.vocabulary['<bos>']\n",
        "\n",
        "        for i, char in enumerate(sentence):\n",
        "            sentence_ids[i + 1] = self.vocabulary.get(char, self.vocabulary['<unk>'])\n",
        "            \n",
        "        # End with <eos>\n",
        "        sentence_ids[len(sentence) + 1] = self.vocabulary['<eos>']\n",
        "\n",
        "        return torch.from_numpy(sentence_ids).long()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.questions)\n",
        "        \n",
        "        q, a = self.questions[idx], self.answers[idx]\n",
        "\n",
        "        question = self.convert_chars_to_ids(q, self.max_len_question)\n",
        "        answer = self.convert_chars_to_ids(a, self.max_len_answer)\n",
        "        \n",
        "        return question, answer"
      ],
      "metadata": {
        "id": "WDyOaDodEfWE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d2 = Mathematics_Dataset(module_files, v)"
      ],
      "metadata": {
        "id": "YDOrfk6diob6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# d2[8]"
      ],
      "metadata": {
        "id": "TzEJ-qROOA6e"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mathematics_DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, modules: List[str], difficulty: List[str]=None, batch_size: int=32):\n",
        "        super().__init__()\n",
        "        self.modules = modules\n",
        "        self.batch_size = batch_size\n",
        "        self.load_vocabulary()\n",
        "\n",
        "        self.train_modules_paths = get_train_module_paths(self.modules, difficulty)  \n",
        "        self.test_modules_paths = get_test_module_paths(self.modules)        \n",
        "\n",
        "    \n",
        "    def load_vocabulary(self):\n",
        "        with open('./datasets/vocabulary.pkl', 'rb') as f:\n",
        "            v = pickle.load(f)\n",
        "        self.vocabulary = create_vocabulary_from_set(v)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"fit\":\n",
        "            self.math_train = Mathematics_Dataset(self.train_modules_paths, self.vocabulary)\n",
        "            self.math_val = Mathematics_Dataset(self.test_modules_paths, self.vocabulary)\n",
        "\n",
        "        if stage == \"test\":\n",
        "            self.math_test = Mathematics_Dataset(self.test_modules_paths, self.vocabulary)\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.math_train, batch_size=self.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):                                                              \n",
        "        return DataLoader(self.math_val, batch_size=self.batch_size, num_workers=2, pin_memory=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.math_test, batch_size=self.batch_size)\n",
        "\n",
        "    def teardown(self, stage: str):\n",
        "        # Used to clean-up when the run is finished\n",
        "        pass"
      ],
      "metadata": {
        "id": "QVBvNs1QisOI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dm = Mathematics_DataModule(['algebra__linear_1d'], batch_size = 64)"
      ],
      "metadata": {
        "id": "nPg1DDjritNc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "ho9Tk3GctrvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, sqrt_q, mask, dropout_layer = None):\n",
        "    t = torch.matmul(query, key.transpose(-2, -1)) / sqrt_q\n",
        "    \"\"\"\n",
        "    t [batch_size, self.num_heads, query.size(-2), key.size(-2)]\n",
        "    mask [batch_size, self.num_head, 1 or query.size(-2), key.size(-2)]\n",
        "    \"\"\"\n",
        "    t = t.masked_fill(mask == False, float(\"-inf\"))\n",
        "    t = F.softmax(t, dim = -1)\n",
        "    if dropout_layer is not None:\n",
        "        t = dropout_layer(t)\n",
        "    return torch.matmul(t, value)"
      ],
      "metadata": {
        "id": "9JbWbeOJtt02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module): \n",
        "    def __init__(self, embedding_dim, num_heads, dropout = 0.2, tp_attention = False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        self.tp_attention = tp_attention\n",
        "        self.dim_head = embedding_dim // num_heads #single head dimension\n",
        "        self.sqrt_q = math.sqrt(self.dim_head)\n",
        "        self.num_heads = num_heads\n",
        "        self.W_q = nn.Linear(embedding_dim, embedding_dim, bias = True) #stack of num_heads matrices of dimension (d, dim_head), one for each head\n",
        "        self.W_k = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        self.W_v = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        self.W_o = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        if self.tp_attention:\n",
        "            self.W_r = nn.Linear(embedding_dim, embedding_dim, bias = True) #ruolo\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #self.dropout = nn.Dropout(0.15)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.W_q.weight)\n",
        "        nn.init.xavier_uniform_(self.W_k.weight)\n",
        "        nn.init.xavier_uniform_(self.W_v.weight)\n",
        "        nn.init.xavier_uniform_(self.W_o.weight)\n",
        "\n",
        "        if self.tp_attention:\n",
        "            nn.init.normal_(self.W_r.weight, mean=0, std=1./self.sqrt_q)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask): #query, key, value\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        q = self.W_q(query).view(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        k = self.W_k(key).view(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        v = self.W_v(value).view(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "\n",
        "        \"\"\"\n",
        "        in the encoder:\n",
        "            q,k,v ([batch_size, self.num_heads, max_len_question, self.dim_head])\n",
        "            mask (src_mask): [batch_size, 1, 1, max_len_question]\n",
        "\n",
        "        in the decoder (MASKED MULTI-HEAD ATTENTION):\n",
        "            seq_len = current_len_answer if inference else max_len_answer\n",
        "                q,k,v ([batch_size, self.num_head, seq_len, self.dim_head])\n",
        "                mask (trg_mask): [batch_size, 1, seq_len, current_len_answer]\n",
        "                \n",
        "        in the decoder (MULTI-HEAD ATTENTION):\n",
        "            seq_len = current_len_answer if inference else max_len_answer\n",
        "                q ([batch_size, self.num_head, seq_len, self.dim_head])\n",
        "                k,v ([batch_size, self.num_head, max_len_question, self.dim_head])\n",
        "                mask (src_mask): [batch_size, 1, 1, max_len_question]\n",
        "        \"\"\"\n",
        "\n",
        "        attention_value = scaled_dot_product_attention(q, k, v, self.sqrt_q, mask, self.dropout)\n",
        "            #attention_value ([batch_size, self.num_heads, q.size(-2), v.size(-1)])\n",
        "\n",
        "        \n",
        "        if self.tp_attention:\n",
        "            role = self.W_r(query).view(batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "            attention_value *= role  #element-wise product between attention value and role before the final projection\n",
        "        return self.W_o(attention_value.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.dim_head))\n",
        "            #output : ([batch_size, q.size(-2)=query.size(-2),embedding_dim)])"
      ],
      "metadata": {
        "id": "cdtptdthS6Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, hidden_size = None, dropout=0.2, tp_attention = False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = MultiHeadAttention(embedding_dim, num_heads, dropout, tp_attention)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        \n",
        "        hidden_size = 4*embedding_dim if hidden_size is None else hidden_size\n",
        "        self.ff = nn.Sequential(nn.Linear(embedding_dim, hidden_size, bias = True), \n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(hidden_size, embedding_dim, bias = True))\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.ff:\n",
        "            if isinstance(p, nn.Linear):\n",
        "                nn.init.xavier_uniform_(p.weight)\n",
        "                if p.bias is not None:\n",
        "                    nn.init.constant_(p.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask): #query, key, value\n",
        "        \"\"\"\n",
        "        if this is a TransformerBlock of the encoder:\n",
        "            query, key, value = x ([batch_size, max_len_question, embedding_dim])\n",
        "\n",
        "        if this is a TransformerBlock of the decoder:\n",
        "            seq_len = current_len_answer if inference else max_len_answer\n",
        "            MASKED MULTI HEAD ATTENTION:\n",
        "                query, key, value = y ([batch_size, seq_len, embedding_dim])\n",
        "            MULTI HEAD ATTENTION:\n",
        "                query: ([batch_size, seq_len, embedding_dim])\n",
        "                key, value: ([batch_size, max_len_question, embedding_dim])\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        x = query + self.dropout1(self.attention(query, key, value, mask)) #query as res conn because the decoder block requires it and it doesn't matter for encoder blocks\n",
        "        x = self.norm1(x)\n",
        "        x = x + self.dropout2(self.ff(x))\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "GhrmQH8sUHwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, hidden_size, dropout = 0.2, tp_attention = False):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.masked_attention = MultiHeadAttention(embedding_dim, num_heads, dropout, tp_attention)\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.transformer_block = TransformerBlock(embedding_dim, num_heads, hidden_size, dropout, tp_attention)\n",
        "\n",
        "    def forward(self, output_encoder, src_mask, y, trg_mask):\n",
        "        y = y + self.dropout(self.masked_attention(y, y, y, trg_mask)) #masked attention (y = query = key = value) + residual connection\n",
        "        y = self.norm(y)\n",
        "        return self.transformer_block(y, output_encoder, output_encoder, src_mask)#query from the masked mha and key and value from the encoder"
      ],
      "metadata": {
        "id": "w0MIvv-ZWL-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=256):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + Variable(self.pe[:, :x.size(1)], requires_grad = False)"
      ],
      "metadata": {
        "id": "qrsDFyoyUF0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, hidden_size, dropout, num_blocks = 6, tp_attention = False):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.encoder = nn.ModuleList(\n",
        "            [TransformerBlock(embedding_dim, num_heads, hidden_size, dropout, tp_attention) for _ in range(num_blocks)]\n",
        "            )\n",
        "\n",
        "    def forward(self, x, mask): \n",
        "        # x ([batch_size, max_len_question, embedding_dim])\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x, x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "bgNDRuG5YIWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, hidden_size, dropout = 0.2, num_blocks = 6, tp_attention = False):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.decoder = nn.ModuleList(\n",
        "            [DecoderBlock(embedding_dim, num_heads, hidden_size, dropout, tp_attention) for _ in range(num_blocks)]\n",
        "            )\n",
        "\n",
        "    def forward(self, output_encoder, src_mask, y, trg_mask): \n",
        "        for block in self.decoder:\n",
        "            y = block(output_encoder, src_mask, y, trg_mask)\n",
        "        return y"
      ],
      "metadata": {
        "id": "Aybtz4uUY8vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self, \n",
        "        special_idxs: Dict[str, int], \n",
        "        optimizer_params: dict,\n",
        "        learning_rate: float=1e-4,\n",
        "        num_heads: int=4, \n",
        "        embedding_dim: int=256, \n",
        "        hidden_size: int=512, \n",
        "        vocabulary_size: int=58,\n",
        "        max_len_question: int=162,\n",
        "        max_len_answer: int=32,\n",
        "        num_blocks_encoder: int=6, \n",
        "        num_blocks_decoder: int=6, \n",
        "        dropout: float=0.2, \n",
        "        gradient_clip_val: float=0.9,\n",
        "        tp_attention: bool=False\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.bos_id = special_idxs['<bos>']\n",
        "        self.eos_id = special_idxs['<eos>']\n",
        "        self.pad_id = special_idxs['<pad>']\n",
        "        self.optimizer_params = optimizer_params\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        \n",
        "        self.token_embedding = nn.Embedding(vocabulary_size, embedding_dim, padding_idx=self.pad_id)\n",
        "        self.positional_embedding = PositionalEncoding(embedding_dim)\n",
        "        self.encoder = TransformerEncoder(embedding_dim, num_heads, hidden_size, dropout, num_blocks_encoder, tp_attention)\n",
        "        self.decoder = TransformerDecoder(embedding_dim, num_heads, hidden_size, dropout, num_blocks_decoder, tp_attention)\n",
        "        self.to_logits = nn.Linear(embedding_dim, vocabulary_size)\n",
        "        \n",
        "        self.max_len_question = max_len_question\n",
        "        self.max_len_answer = max_len_answer\n",
        "\n",
        "        self.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "        self.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "        self.test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, \n",
        "                        mean=0, std=1./math.sqrt(self.embedding_dim))\n",
        "        \n",
        "        nn.init.normal_(self.to_logits.weight, \n",
        "                        mean=0, std=1./math.sqrt(self.vocabulary_size))\n",
        "\n",
        "        if self.to_logits.bias is not None:\n",
        "            nn.init.constant_(self.to_logits.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "    def create_trg_mask(self, y): #compute a mask so that the prediction of the next token can only depend on the previous tokens\n",
        "        # #[batch_size, 1, len, len] & [batch_size, 1, 1, len]\n",
        "        return self.create_causal_mask(y) & self.create_padding_mask(y)\n",
        "\n",
        "\n",
        "    def create_causal_mask(self, y):\n",
        "        batch_size, seq_len = y.shape\n",
        "        mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device = self.device)).expand(\n",
        "            batch_size, 1, seq_len, seq_len)\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def create_padding_mask(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        mask = (x != self.pad_id).unsqueeze(-2).unsqueeze(-2)\n",
        "        return mask\n",
        "\n",
        "    def greedy_decode(self,x):\n",
        "        batch_size = x.size(0)\n",
        "        src_mask = self.create_padding_mask(x)\n",
        "        # src_mask ([batch_size, 1, 1, self.max_len_question]), \n",
        "                #la dimensione [-2] è 1 perché per ogni token della domanda la maschera è la stessa (broadcasting)\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.positional_embedding(x)\n",
        "        #x ([batch_size, self.max_len_question, self.embedding_dim])\n",
        "\n",
        "        output_encoder = self.encoder(x, src_mask)\n",
        "        #output_encoder : ([batch_size, self.max_len_question, embedding_dim]) \n",
        "\n",
        "        output = torch.ones(batch_size, 1, dtype=torch.int64, device = self.device).fill_(self.bos_id)\n",
        "        #output: ([batch_size, 1]) \n",
        "        done = torch.zeros(batch_size, dtype = torch.uint8, device = self.device)\n",
        "        for _ in range(self.max_len_answer - 1): \n",
        "            trg_mask = self.create_trg_mask(output)\n",
        "            # tgr_mask ([batch_size, 1, len_current_answer, len_current_answer])\n",
        "\n",
        "            output_embedding = self.token_embedding(output)\n",
        "            output_embedding = self.positional_embedding(output_embedding)\n",
        "            #output_embedding ([batch_size, len_current_answer, self.embedding_dim])\n",
        "\n",
        "            out = self.decoder(output_encoder, src_mask, output_embedding, trg_mask)\n",
        "            #out ([batch_size, len_current_answer, self.embedding_dim])\n",
        "            out = self.to_logits(out)\n",
        "            #out ([batch_size, len_current_answer, self.vocabulary_size])\n",
        "            out = torch.argmax(out[:,[-1],:], dim = -1)\n",
        "            output = torch.cat([output, out], dim = 1) #we concatenate the new token to the output answer\n",
        "            eos_reached = out.squeeze(1) == self.eos_id\n",
        "            done |= eos_reached\n",
        "            if done.sum() == batch_size:\n",
        "                break\n",
        "        return output\n",
        "\n",
        "\n",
        "    def inference(self, x):\n",
        "        #encode and then generate the output token by token greedily\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            return self.greedy_decode(x)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \n",
        "        # x ([batch_size, self.max_len_question])\n",
        "        # y ([batch_size, self.max_len_answer])\n",
        "\n",
        "        src_mask = self.create_padding_mask(x)\n",
        "\n",
        "        # src_mask ([batch_size, 1, 1, self.max_len_question]),\n",
        "                    #la dimensione [-3] è 1 perché successivamente viene effettuato broadcasting per ogni head della MULTI-HEAD ATTENTION \n",
        "                    #la dimensione [-2] è 1 perché per ogni token della domanda la maschera è la stessa (broadcasting)\n",
        "\n",
        "        trg_mask = self.create_trg_mask(y)\n",
        "\n",
        "        # tgr_mask ([batch_size, 1, self.max_len_answer-1, self.max_len_answer-1]),\n",
        "                    #la dimensione [-3] è 1 perché successivamente viene effettuato broadcasting per ogni head della MULTI-HEAD ATTENTION \n",
        "                    #la dimensione [-2] è self.max_len_answer perché per ogni token della domanda la maschera è diversa (maschera causale)\n",
        "\n",
        "\n",
        "        x = self.token_embedding(x)\n",
        "\n",
        "        x = self.positional_embedding(x)\n",
        "\n",
        "        #x ([batch_size, self.max_len_question, self.embedding_dim])\n",
        "\n",
        "        y = self.token_embedding(y)\n",
        "        y = self.positional_embedding(y)\n",
        "\n",
        "        #y ([batch_size, self.max_len_answer-1, self.embedding_dim])\n",
        "\n",
        "        output_encoder = self.encoder(x, src_mask)\n",
        "\n",
        "        #output_encoder : ([batch_size, self.max_len_question, embedding_dim])        \n",
        "\n",
        "        output_decoder = self.decoder(output_encoder, src_mask, y, trg_mask)\n",
        "\n",
        "        #output_decoder : ([batch_size, self.max_len_answer-1, embedding_dim]))\n",
        "\n",
        "        return self.to_logits(output_decoder).transpose(1,2)\n",
        "    \n",
        "    def configure_optimizers(self):# learning rate = 1x10^-4; beta1 =0.9; beta2 = 0.995 dal paper\n",
        "        betas = self.optimizer_params['betas']\n",
        "        return torch.optim.Adam(self.parameters(), self.learning_rate, betas)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self(x, y[:, :-1])\n",
        "        loss = F.cross_entropy(y_pred, y[:, 1:], ignore_index = self.pad_id)\n",
        "        \n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        if batch_idx % 1000 == 0:\n",
        "            y_pred2 = self.inference(x)\n",
        "            y_pred2 = F.pad(y_pred2, (0, self.max_len_answer - y_pred2.shape[1]), mode='constant', value=self.pad_id)\n",
        "            self.train_accuracy.update(y_pred2[:, 1:], y[:, 1:])\n",
        "            self.log('train_accuracy_forward', self.train_accuracy.compute(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x)  #[batch_size, max_eos_found]\n",
        "        y_pred = F.pad(y_pred, (0, self.max_len_answer - y_pred.shape[1]), mode='constant', value=self.pad_id) #[batch_size, max_len_answer]\n",
        "        self.val_accuracy.update(y_pred[:, 1:], y[:, 1:]) #y_pred, y nel caso volessimo contare <bos> come carattere corretto\n",
        "        self.log('val_accuracy_step', self.val_accuracy.compute(), on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x)  #[batch_size, max_eos_found]\n",
        "        y_pred = F.pad(y_pred, (0, self.max_len_answer - y_pred.shape[1]), mode='constant', value=self.pad_id) #[batch_size, max_len_answer]\n",
        "        self.test_accuracy.update(y_pred[:, 1:], y[:, 1:])\n",
        "        \n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log('val_accuracy_epoch', self.val_accuracy.compute(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.val_accuracy.reset()\n",
        "        \n",
        "        # Also reset the training accuracy\n",
        "        self.train_accuracy.reset()\n",
        "    \n",
        "    def on_test_epoch_end(self):\n",
        "        self.log('test_accuracy_epoch', self.test_accuracy.compute(), on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.test_accuracy.reset()\n"
      ],
      "metadata": {
        "id": "qiPC8tqNY8wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocabulary = v\n",
        "LEARNING_RATE = 1e-4 # 2.558585886905645e-05\n",
        "BATCH_SIZE = 128\n",
        "EMBEDDING_DIM = 16\n",
        "NUM_HEADS = 4\n",
        "assert EMBEDDING_DIM % NUM_HEADS == 0\n",
        "# HIDDEN_SIZE = 2048\n",
        "HIDDEN_SIZE = 16\n",
        "DROP_PROB = 0.5\n",
        "GRADIENT_CLIP_VAL = 0.5\n",
        "NUM_BLOCKS_ENCODER = 1\n",
        "NUM_BLOCKS_DECODER = 1\n",
        "SPECIAL_CHAR_DICT = {'<bos>': vocabulary['<bos>'], '<eos>': vocabulary['<eos>'], '<pad>': vocabulary['<pad>']}\n",
        "OPTIMIZER_PARAMS = {'betas': (0.9, 0.995)}\n",
        "\n",
        "tp_transformer_hyperparams = {\n",
        "    \"special_idxs\": SPECIAL_CHAR_DICT,\n",
        "    \"optimizer_params\": OPTIMIZER_PARAMS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"num_heads\": NUM_HEADS,\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"vocabulary_size\": len(vocabulary),\n",
        "    \"num_blocks_encoder\": NUM_BLOCKS_ENCODER,\n",
        "    \"num_blocks_decoder\": NUM_BLOCKS_DECODER,\n",
        "    \"dropout\": DROP_PROB,\n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL, # Added just to be saved\n",
        "    \"tp_attention\": True\n",
        "}\n",
        "\n",
        "tp_transformer = Transformer(**tp_transformer_hyperparams)\n",
        "modules = ['algebra__linear_1d']\n",
        "math_dm = Mathematics_DataModule(modules, batch_size=4)\n"
      ],
      "metadata": {
        "id": "lSsMh3G73IZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOTA"
      ],
      "metadata": {
        "id": "sEka3B3Eq69g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = v"
      ],
      "metadata": {
        "id": "n-5i8lY1sEIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"Luigi\"\n",
        "\n",
        "root_dir = \"./training/checkpoints\"\n",
        "logger_dir = \"./training/tensorboard/logs_tp_transformer\"\n",
        "checkpoint_dir = \"./training/checkpoints/PRO_\" + name + \"_tp_transformer_checkpoints\"\n",
        "#checkpoint_dir = \"./training/checkpoints/Standard_Luigi_tp_transformer_checkpoints\"\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4 # 2.558585886905645e-05\n",
        "BATCH_SIZE = 256\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_HEADS = 8\n",
        "assert EMBEDDING_DIM % NUM_HEADS == 0\n",
        "\n",
        "DROP_PROB = 0.3\n",
        "GRADIENT_CLIP_VAL = 0.5\n",
        "NUM_BLOCKS_ENCODER = 3\n",
        "NUM_BLOCKS_DECODER = 3\n",
        "SPECIAL_CHAR_DICT = {'<bos>': vocabulary['<bos>'], '<eos>': vocabulary['<eos>'], '<pad>': vocabulary['<pad>']}\n",
        "OPTIMIZER_PARAMS = {'betas': (0.9, 0.995)}"
      ],
      "metadata": {
        "id": "0aPW_6wrs_IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp_transformer_hyperparams = {\n",
        "    \"special_idxs\": SPECIAL_CHAR_DICT,\n",
        "    \"optimizer_params\": OPTIMIZER_PARAMS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"num_heads\": NUM_HEADS,\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"vocabulary_size\": len(vocabulary),\n",
        "    \"num_blocks_encoder\": NUM_BLOCKS_ENCODER,\n",
        "    \"num_blocks_decoder\": NUM_BLOCKS_DECODER,\n",
        "    \"dropout\": DROP_PROB,\n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL, # Added just to be saved\n",
        "    \"tp_attention\": True\n",
        "}\n",
        "\n",
        "now = datetime.now().strftime(\"%H.%M\")\n",
        "\n",
        "logger = TensorBoardLogger(logger_dir, name=\"PRO_tp_transformer\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = checkpoint_dir,\n",
        "    filename=now+'tp_transformer_{epoch:02d}_{step:06d}_{val_accuracy_epoch:.3f}',\n",
        "    save_top_k=6,\n",
        "    monitor='val_accuracy_epoch',\n",
        "    mode='max',\n",
        "    verbose=True,\n",
        "    save_last=True\n",
        ")\n",
        "callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "trainer_hyperparams = {\n",
        "    \"default_root_dir\": root_dir,\n",
        "    \"accelerator\": \"auto\",\n",
        "    \"devices\": 1,\n",
        "    \"precision\": 32, \n",
        "    \"log_every_n_steps\": 100,\n",
        "    # \"val_check_interval\": 0.5, # validation step called 2 times during a training epoch\n",
        "    \"val_check_interval\": 1.0, \n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL,\n",
        "    \"max_epochs\": EPOCHS,\n",
        "    \"logger\": logger,\n",
        "    \"callbacks\": callbacks,\n",
        "    # \"deterministic\": True,\n",
        "}\n",
        "\n",
        "modules = ['algebra__linear_1d', 'probability__swr_p_level_set', 'numbers__is_prime']\n",
        "#modules = ['algebra__linear_1d']\n",
        "math_dm = Mathematics_DataModule(modules, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mWWvh1A5m5-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp_transformer = Transformer(**tp_transformer_hyperparams)\n",
        "trainer = Trainer(**trainer_hyperparams)\n",
        "\n",
        "trainer.fit(tp_transformer, datamodule=math_dm)\n",
        "\"\"\"\n",
        "math_dm.setup(\"fit\")\n",
        "trainer.fit(tp_transformer, train_dataloaders=math_dm.train_dataloader(), val_dataloaders=math_dm.val_dataloader())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Uf3QJeF-q-AW",
        "outputId": "cbefb682-2a04-45aa-a526-17144f74ced2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "a9b2fa9cf2874620ae76b1d533cc6f90",
            "cabcdcabfd82419ba7a5c11a8db435ab",
            "7a5393e2277947feaaa5f53bc501a82c",
            "25fb20ed6b8a41248d6f68f369233b6e",
            "ccd588a4e2ab46e182d7b64e8f0802b2",
            "dcf76247b5a24fc9a844d51ee2890277",
            "99cd371ce4ee4efb98dd730782155525",
            "4e7c028a11494dabb457560ded31f84d",
            "ffb4fb847b5e48a9a2858c3a4deb7c2a",
            "6bd3acd079cc4f959886e05cd6516029",
            "65b9daf0ed0d4be88bf04354864dc3f8",
            "6831056493ac42438957796550288ca7",
            "b9d1e76ecd4a4105b1c52cce361afa43",
            "07c6e71e82754757ab5c883548e1b245",
            "f284487f9aea4c56bffd87ac8ba59316",
            "b1c6524a347f4b42b002ca5e65ab0e8f",
            "7f17aa809f3941fe820451453dad9638",
            "1da508a84f0d4530a28d981257e0ee39",
            "f2b71dc7c2d94c66b081336ebbf51b36",
            "3a625391db284596a1acc34c8438cc17",
            "a8ca3c874b724e99ac1309359ae50383",
            "31e66d237d93492980c81e6c93e5b7e3"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/drive/MyDrive/Deep_Learning_Project/training/checkpoints/PRO_Luigi_tp_transformer_checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                 | Type               | Params\n",
            "------------------------------------------------------------\n",
            "0 | token_embedding      | Embedding          | 14.8 K\n",
            "1 | positional_embedding | PositionalEncoding | 0     \n",
            "2 | encoder              | TransformerEncoder | 1.4 M \n",
            "3 | decoder              | TransformerDecoder | 2.4 M \n",
            "4 | to_logits            | Linear             | 14.9 K\n",
            "5 | train_accuracy       | MulticlassAccuracy | 0     \n",
            "6 | val_accuracy         | MulticlassAccuracy | 0     \n",
            "7 | test_accuracy        | MulticlassAccuracy | 0     \n",
            "------------------------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.8 M     Total params\n",
            "15.150    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9b2fa9cf2874620ae76b1d533cc6f90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6831056493ac42438957796550288ca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmath_dm.setup(\"fit\")\\ntrainer.fit(tp_transformer, train_dataloaders=math_dm.train_dataloader(), val_dataloaders=math_dm.val_dataloader())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_hyperparams = torch.load(f\"{checkpoint_dir}/last-v3.ckpt\")\n",
        "trained_hyperparams['hyper_parameters']"
      ],
      "metadata": {
        "id": "vzfHUGm_xyH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = Transformer(**tp_transformer_hyperparams)\n",
        "test_trainer = Trainer(**trainer_hyperparams)\n",
        "\"\"\"\n",
        "# test_trainer.test(test_model, datamodule=math_dm, ckpt_path=f\"{checkpoint_dir}/tp_transformer_epoch=00_step=001464_val_accuracy_epoch=0.287.ckpt\", verbose=True)\n",
        "test_trainer.test(test_model, datamodule=math_dm, ckpt_path=\"best\", verbose=True) # ckpt_path=\"best\"\n",
        "\"\"\"\n",
        "math_dm.setup(\"test\")\n",
        "test_trainer.test(test_model, dataloaders = math_dm.test_dataloader(), ckpt_path=\"best\", verbose=True)"
      ],
      "metadata": {
        "id": "IZZflQpInBdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./training/tensorboard/logs/TP-Transformer #modifica in base al tuo path"
      ],
      "metadata": {
        "id": "-TeR1elp0lzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = \"./training/checkpoints/tp_transformer_checkpoints\"  \n",
        "logger = TensorBoardLogger(logger_dir, name=\"TP-Transformer\", log_graph=True)\n",
        "\n",
        "\n",
        "\n",
        "ckpt_path = checkpoint_dir + \"/last.ckpt\"  #attenzione che in caso di nuovi last checkpoint il nome è diverso\n",
        "checkpoint_dir_fineTuning = \"./training/checkpoints/tp_transformer_checkpoints_fineTuning\"\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = checkpoint_dir_fineTuning,\n",
        "    filename='tp_transformer_{epoch:02d}_{step:06d}',\n",
        "    save_top_k=3,\n",
        "    monitor='accuracy_epoch',\n",
        "    mode='max',\n",
        "    save_last=True\n",
        ")\n",
        "tp_transformer_ckpt = Transformer.load_from_checkpoint(ckpt_path)\n",
        "\n",
        "\n",
        "callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "\n",
        "trainer = pl.Trainer(log_every_n_steps=1, default_root_dir=root_dir, accelerator='auto', devices=1, gradient_clip_val = 0.1, max_epochs = EPOCHS + ADDITIONAL_EPOCHS, logger = logger, callbacks = callbacks)\n",
        "math_dm = Mathematics_DataModule(['algebra__linear_1d'], batch_size = BATCH_SIZE)\n",
        "trainer.fit(tp_transformer_ckpt, datamodule = math_dm, ckpt_path = ckpt_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "tPoVmsfdFUcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NON-SOTA (Transformer)"
      ],
      "metadata": {
        "id": "0b4LEKhjtizS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = v"
      ],
      "metadata": {
        "id": "RnAfZahj3PuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"./training/checkpoints\"\n",
        "logger_dir = \"./training/tensorboard/logs\"\n",
        "checkpoint_dir = \"./training/checkpoints/transformer_vanilla_checkpoints.ckpt\"\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 128\n",
        "EMBEDDING_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "assert EMBEDDING_DIM % NUM_HEADS == 0\n",
        "HIDDEN_SIZE = 256\n",
        "DROP_PROB = 0.3\n",
        "NUM_BLOCKS_ENCODER = 3\n",
        "NUM_BLOCKS_DECODER = 3\n",
        "SPECIAL_CHAR_DICT = {'<bos>': vocabulary['<bos>'], '<eos>': vocabulary['<eos>'], '<pad>': vocabulary['<pad>']}\n",
        "GRADIENT_CLIP_VAL = 0.5\n",
        "OPTIMIZER_PARAMS = {'betas': (0.9, 0.995)}\n",
        "ADDITIONAL_EPOCHS = 5"
      ],
      "metadata": {
        "id": "cmfS_iAe357E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_hyperparams = {\n",
        "    \"special_idxs\": SPECIAL_CHAR_DICT,\n",
        "    \"optimizer_params\": OPTIMIZER_PARAMS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"num_heads\": NUM_HEADS,\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"vocabulary_size\": len(vocabulary),\n",
        "    \"num_blocks_encoder\": NUM_BLOCKS_ENCODER,\n",
        "    \"num_blocks_decoder\": NUM_BLOCKS_DECODER,\n",
        "    \"dropout\": DROP_PROB,\n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL, # Added just to be saved\n",
        "}\n",
        "\n",
        "now = datetime.now().strftime(\"%H.%M\")\n",
        "\n",
        "logger = TensorBoardLogger(logger_dir, name=\"transformer\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = checkpoint_dir,\n",
        "    filename=now+'transformer_{epoch:02d}_{step:06d}_{val_accuracy_epoch:.3f}',\n",
        "    save_top_k=6,\n",
        "    monitor='val_accuracy_epoch',\n",
        "    mode='max',\n",
        "    verbose=True,\n",
        "    save_last=True\n",
        ")\n",
        "callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "trainer_hyperparams = {\n",
        "    \"default_root_dir\": root_dir,\n",
        "    \"accelerator\": \"auto\",\n",
        "    \"devices\": 1,\n",
        "    \"precision\": 32, \n",
        "    \"log_every_n_steps\": 100,\n",
        "    # \"val_check_interval\": 0.5, # validation step called 2 times during a training epoch\n",
        "    \"val_check_interval\": 1.0, \n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL,\n",
        "    \"max_epochs\": EPOCHS,\n",
        "    \"logger\": logger,\n",
        "    \"callbacks\": callbacks,\n",
        "    # \"deterministic\": True,\n",
        "}\n",
        "\n",
        "modules = ['algebra__linear_1d', 'probability__swr_p_level_set', 'numbers__is_prime']\n",
        "#modules = ['algebra__linear_1d']\n",
        "math_dm = Mathematics_DataModule(modules, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "nqywWO1gUxwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(**transformer_hyperparams)\n",
        "trainer = Trainer(**trainer_hyperparams)\n",
        "trainer.fit(transformer, datamodule=math_dm)"
      ],
      "metadata": {
        "id": "FtATASTk4PIf",
        "outputId": "8cfa80f4-5077-4e57-ecb3-c21a2d7ff1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438,
          "referenced_widgets": [
            "c97fd8af7e49418190e89b968c36d3ac",
            "30391edb9dc046aaabf11a1acced6633",
            "7a9ef17d4d5a4195a39eafba79ecb5d2",
            "e58146bf0fbb4164a3a677fe354dddc7",
            "32ede6f55519440d8f2f0749584ff0be",
            "747b60403db94b44b88e233320be4f77",
            "df7c75874a0c4533b349c7070a1f080e",
            "7fe3f26b78614837bb378a8954be0fa3",
            "c5d296b3c54e41ff88e01d04f7b4a591",
            "e61d10d2b3ae4cf9a0e6c6772a7932b5",
            "10889d7471be4ce097a22d912a939260",
            "d08519757a134b888b1708c9dc664d00",
            "088497c1bcf2431caa748c8a5ff4fb80",
            "cad77d9228a64539bc66788b907b9f1f",
            "29ba00c1748841f2b6d703482d2db8ca",
            "a3203a43bb394e53b44a573c4f9d1554",
            "0ad3d6411ce547909d5f6f8ee4b8d705",
            "5abc109dedfe489fa7f963b45648aeb7",
            "ab60a4af7f1848b99feb3d5c11d3a68c",
            "62976af232e14041997b6a75872cb677",
            "7cd59ebfb16d4963bd17fcd3c7d49abe",
            "6081337b6ad546d3b9bf9124d371b991"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                 | Type               | Params\n",
            "------------------------------------------------------------\n",
            "0 | token_embedding      | Embedding          | 14.8 K\n",
            "1 | positional_embedding | PositionalEncoding | 0     \n",
            "2 | encoder              | TransformerEncoder | 1.2 M \n",
            "3 | decoder              | TransformerDecoder | 2.0 M \n",
            "4 | to_logits            | Linear             | 14.9 K\n",
            "5 | train_accuracy       | MulticlassAccuracy | 0     \n",
            "6 | val_accuracy         | MulticlassAccuracy | 0     \n",
            "7 | test_accuracy        | MulticlassAccuracy | 0     \n",
            "------------------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.782    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c97fd8af7e49418190e89b968c36d3ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d08519757a134b888b1708c9dc664d00"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"trainer.validate(datamodule=dm)\n",
        "trainer.test(datamodule=dm)\"\"\""
      ],
      "metadata": {
        "id": "XhMRvMTl4ujG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./training/tensorboard/logs/Transformer-Vanilla"
      ],
      "metadata": {
        "id": "PMLeHmow4vNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Seq2Seq"
      ],
      "metadata": {
        "id": "HXt1-jw0jRD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_pytorch(nn.Module):\n",
        "    \n",
        "    def __init__ (self, input_size, hidden_size, num_cells=2, dropout=0.2):\n",
        "        assert num_cells>0\n",
        "\n",
        "        super(LSTM_pytorch, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.num_cells = num_cells\n",
        "        \n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_cells, dropout=dropout, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_cells, batch_first=True)\n",
        "        \n",
        "    def forward(self, x, h = None, cell = None):\n",
        "        \"x è una sequenza [batch, seq_len, embedding_dim]\"\n",
        "\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        \"h [self.num_cells, batch_size, self.hidden_size]\"\n",
        "        if h is None:\n",
        "            output_states, (hidden_states, cells) = self.lstm(x)\n",
        "        else:\n",
        "            output_states, (hidden_states, cells) = self.lstm(x, (h, cell))\n",
        "\n",
        "        return output_states, (hidden_states, cells) #(così si prende output_states[:,-1,:] da dare al decoder e tutto output_states per il linear dopo il decoder)\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "luCNxsG8uFeS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMEncoderDecoder(pl.LightningModule): #oppure Seq2Seq (informiamoci sui nomi)\n",
        "    def __init__(self,\n",
        "        special_idxs: Dict[str, int],\n",
        "        optimizer_params: dict,\n",
        "        learning_rate: float=1e-4,\n",
        "        embedding_dim: float=256,\n",
        "        hidden_size: int=512,\n",
        "        vocabulary_size: int=58,\n",
        "        max_len_question: int=162,\n",
        "        max_len_answer: int=32,\n",
        "        num_cells: int=2,\n",
        "        dropout: float=0.2,\n",
        "        teacher_forcing_ratio: float=0.5\n",
        "    ):\n",
        "        super(LSTMEncoderDecoder, self).__init__()\n",
        "\n",
        "        #CONTROLLARE LE DIMENSIONI\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.bos_id = special_idxs['<bos>']\n",
        "        self.eos_id = special_idxs['<eos>']\n",
        "        self.pad_id = special_idxs['<pad>']\n",
        "\n",
        "        self.optimizer_params = optimizer_params\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "\n",
        "        self.LSTM_encoder = LSTM_pytorch(embedding_dim, hidden_size, num_cells, dropout) \n",
        "\n",
        "        self.LSTM_decoder = LSTM_pytorch(embedding_dim + hidden_size, hidden_size, num_cells, dropout)\n",
        "\n",
        "        self.to_logits = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
        "                                       nn.ReLU(), \n",
        "                                       nn.Dropout(dropout),\n",
        "                                       nn.Linear(hidden_size, vocabulary_size)) \n",
        "        \n",
        "        self.max_len_question = max_len_question\n",
        "        self.max_len_answer = max_len_answer\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "        self.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "        self.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "        self.test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "        \n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, \n",
        "                        mean=0, std=1./math.sqrt(self.embedding_dim))\n",
        "\n",
        "        for p in self.to_logits:\n",
        "            if isinstance(p, nn.Linear):\n",
        "                nn.init.normal_(p.weight, \n",
        "                         mean=0, std=1./math.sqrt(self.vocabulary_size))\n",
        "\n",
        "                if p.bias is not None:\n",
        "                    nn.init.constant_(p.bias, 0)\n",
        "\n",
        "    def greedy_decode(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.token_embedding(x)\n",
        "\n",
        "            # x [batch_size, max_len_answer embedding_dim]\n",
        "        \n",
        "        output_encoder, (previous_state, cell) = self.LSTM_encoder(x)\n",
        "\n",
        "            # output_encoder [batch_size, max_len_answer, hidden_size]\n",
        "\n",
        "            # previous_state is a list [num_cells, batch_size, hidden_size]\n",
        "\n",
        "            #last_state_encoder [batch_size, 1, hidden_size]\n",
        "        last_state_encoder = previous_state[-1].unsqueeze(dim=1)\n",
        "\n",
        "            #output [batch_size, 1]\n",
        "        output = torch.ones(batch_size, 1, dtype=torch.int64, device = self.device).fill_(self.bos_id)\n",
        "\n",
        "        done = torch.zeros(batch_size, dtype = torch.uint8, device = self.device)\n",
        "\n",
        "\n",
        "        for _ in range(self.max_len_answer - 1):\n",
        "            \n",
        "            current_output = output[:,-1].unsqueeze(dim=1)\n",
        "                #current_output [batch_size, 1]\n",
        "            #current_output = output\n",
        "\n",
        "            current_output_embedding = self.token_embedding(current_output)\n",
        "                #current_output_embedding [batch_size, 1, embedding_dim]\n",
        "\n",
        "\n",
        "            input_decoder = torch.cat((last_state_encoder, current_output_embedding), dim=-1)\n",
        "                #input_decoder [batch_size, 1, embedding_dim + hidden_size]\n",
        "                \n",
        "            out, (previous_state, cell) = self.LSTM_decoder(input_decoder, h = previous_state, cell = cell)\n",
        "                #out [batch_size, 1, hidden_size]\n",
        "                #previous_state is a list [num_cells, batch_size, hidden_size]\n",
        "            \n",
        "            out = self.to_logits(out)\n",
        "                #out [batch_size, 1, vocabulary_size]\n",
        "\n",
        "            out = torch.argmax(out[:,[-1],:], dim = -1)\n",
        "                #out [batch_size, 1]\n",
        "\n",
        "\n",
        "            output = torch.cat([output, out], dim = 1)\n",
        "                #output [batch_size, current_len]\n",
        "\n",
        "\n",
        "            eos_reached = out.squeeze(1) == self.eos_id\n",
        "            done |= eos_reached\n",
        "            if done.sum() == batch_size:\n",
        "                break\n",
        "\n",
        "        return output\n",
        "\n",
        "    def inference(self, x):\n",
        "\n",
        "        #encode and then generate the output token by token greedily\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            return self.greedy_decode(x)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        mask_x = x != self.pad_id\n",
        "\n",
        "        x = self.token_embedding(x)\n",
        "            #x [batch_size, max_len_question, embedding_dim]\n",
        "\n",
        "        y = self.token_embedding(y)\n",
        "            #y [batch_size, max_len_answer-1, embedding_dim]\n",
        "\n",
        "        output_encoder, (previous_state, cell) = self.LSTM_encoder(x) \n",
        "\n",
        "            #output_encoder [batch_size, max_len_question, hidden_size]\n",
        "            #state_encoder is a list [num_cells, batch_size, hidden_size]\n",
        "                        \n",
        "        last_state_encoder = previous_state[-1].unsqueeze(dim=1)\n",
        "            #last_state_encoder [batch_size, hidden_size]\n",
        "            \n",
        "        batch_size = y.size(0)\n",
        "        output = torch.ones(batch_size, 1, dtype=torch.int64, device = self.device).fill_(self.bos_id)\n",
        "        output_logits = torch.zeros((batch_size, self.max_len_answer - 1, self.vocabulary_size), device = self.device) \n",
        "        for t in range(self.max_len_answer - 1):\n",
        "            if random.random() < self.teacher_forcing_ratio:\n",
        "                current_output_embedding = y[:, t, :].unsqueeze(dim=1)\n",
        "            else:\n",
        "                current_output_embedding = self.token_embedding(output[:, t].unsqueeze(dim=1)) #output[:, -1]\n",
        "\n",
        "            input_decoder = torch.cat((last_state_encoder, current_output_embedding), dim=-1)\n",
        "                #input_decoder [batch_size, 1, embedding_dim + hidden_size]\n",
        "                \n",
        "            out, (previous_state, cell) = self.LSTM_decoder(input_decoder, h = previous_state, cell = cell)\n",
        "                #out [batch_size, 1, hidden_size]\n",
        "            \n",
        "            out = self.to_logits(out)\n",
        "                #out [batch_size, 1, vocabulary_size]\n",
        "            output_logits[:, t, :] = out.squeeze(dim=1)\n",
        "            out = torch.argmax(out[:,[-1],:], dim = -1)\n",
        "                #out [batch_size, 1]\n",
        "\n",
        "\n",
        "            output = torch.cat([output, out], dim = 1)\n",
        "\n",
        "        #return self.to_logits(output_decoder).transpose(1,2), state_enc\n",
        "        return output_logits.transpose(1,2)\n",
        "        \n",
        "    \n",
        "    def configure_optimizers(self):# learning rate = 1x10^-4; beta1 =0.9; beta2 = 0.995 dal paper\n",
        "        betas = self.optimizer_params['betas']\n",
        "        return torch.optim.Adam(self.parameters(), self.learning_rate, betas)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self(x[:, 1:], y[:, :-1])\n",
        "        loss = F.cross_entropy(y_pred, y[:, 1:], ignore_index = self.pad_id)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        self.train_accuracy.update(y_pred, y[:, 1:])\n",
        "        self.log('train_accuracy_epoch', self.train_accuracy.compute(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x[:, 1:])  #[batch_size, max_eos_found]\n",
        "        y_pred = F.pad(y_pred, (0, self.max_len_answer - y_pred.shape[1]), mode='constant', value=self.pad_id) #[batch_size, max_len_answer]\n",
        "        self.val_accuracy.update(y_pred[:, 1:], y[:, 1:])\n",
        "        self.log('val_accuracy_step', self.val_accuracy.compute(), on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x[:, 1:])  #[batch_size, max_eos_found]\n",
        "        y_pred = F.pad(y_pred, (0, self.max_len_answer - y_pred.shape[1]), mode='constant', value=self.pad_id) #[batch_size, max_len_answer]\n",
        "\n",
        "        self.test_accuracy.update(y_pred[:, 1:], y[:, 1:])\n",
        "        \n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log('val_accuracy_epoch', self.val_accuracy.compute(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.val_accuracy.reset()\n",
        "        \n",
        "        # Also reset the training accuracy\n",
        "        self.train_accuracy.reset()\n",
        "\n",
        "    \n",
        "    def on_test_epoch_end(self):\n",
        "        self.log('test_accuracy_epoch', self.test_accuracy.compute(), on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.test_accuracy.reset()\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "4Zuv3MbWvQ1h"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NON SOTA (LSTM)"
      ],
      "metadata": {
        "id": "X963rmZkD9_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = v"
      ],
      "metadata": {
        "id": "adgjWnpxEYQm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"./training/LSTM/checkpoints\"\n",
        "logger_dir = \"./training/LSTM/tensorboard/logs\"\n",
        "checkpoint_dir = \"./training/LSTM/checkpoints/lstm_seq2seq_checkpoints\"\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "DROP_PROB = 0.3\n",
        "GRADIENT_CLIP_VAL = 0.5\n",
        "\n",
        "NUM_CELLS = 2\n",
        "SPECIAL_CHAR_DICT = {'<bos>': vocabulary['<bos>'], '<eos>': vocabulary['<eos>'], '<pad>': vocabulary['<pad>']}\n",
        "OPTIMIZER_PARAMS = {'betas': (0.9, 0.995)}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WWl2phutMGV9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_hyperparams = {\n",
        "    \"special_idxs\": SPECIAL_CHAR_DICT,\n",
        "    \"optimizer_params\": OPTIMIZER_PARAMS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"vocabulary_size\": len(vocabulary),\n",
        "    \"num_cells\": NUM_CELLS,\n",
        "    \"dropout\": DROP_PROB,\n",
        "    \"teacher_forcing_ratio\": 0.5\n",
        "    }\n",
        "\n",
        "logger = TensorBoardLogger(logger_dir, name=\"LSTM\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = checkpoint_dir,\n",
        "    filename='LSTM_{epoch:02d}_{step:06d}_{val_accuracy_epoch:.3f}',\n",
        "    save_top_k=6,\n",
        "    monitor='val_accuracy_epoch',\n",
        "    mode='max',\n",
        "    verbose=True,\n",
        "    save_last=True\n",
        ")\n",
        "callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "trainer_hyperparams = {\n",
        "    \"default_root_dir\": root_dir,\n",
        "    \"accelerator\": \"auto\",\n",
        "    \"devices\": 1,\n",
        "    \"precision\": 32, \n",
        "    \"log_every_n_steps\": 100,\n",
        "    # \"val_check_interval\": 0.5, # validation step called 2 times during a training epoch\n",
        "    \"val_check_interval\": 1.0, \n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL,\n",
        "    \"max_epochs\": EPOCHS,\n",
        "    \"logger\": logger,\n",
        "    \"callbacks\": callbacks,\n",
        "    # \"deterministic\": True,\n",
        "}\n",
        "\n",
        "modules = ['algebra__linear_1d', 'probability__swr_p_level_set', 'numbers__is_prime']\n",
        "#modules = ['algebra__linear_1d']\n",
        "math_dm = Mathematics_DataModule(modules, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Rzd0sHINEwC0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "lstm_seq2seq = LSTMEncoderDecoder(**LSTM_hyperparams)\n",
        "\n",
        "math_dm.setup('fit')\n",
        "for batch in math_dm.train_dataloader():\n",
        "    x, y = batch\n",
        "    mask_x = (x[:, 1:] != 0)[0]\n",
        "    y_pred = lstm_seq2seq(x, y[:,:-1])\n",
        "    break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yPG3xwKTdjS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_seq2seq = LSTMEncoderDecoder(**LSTM_hyperparams)\n",
        "\n",
        "\n",
        "trainer = Trainer(**trainer_hyperparams)\n",
        "\n",
        "trainer.fit(lstm_seq2seq, datamodule=math_dm)"
      ],
      "metadata": {
        "id": "qsOQXljYMGWC",
        "outputId": "21e245a4-e142-44c5-baa6-4ad03ef69f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "747b77f860224de39a44cdb646710024",
            "4283f9d98e2543ed9e9eaf393604da9d",
            "2aa026eeb61b430e93e75fb9f63f12ad",
            "f62eca98f001462bbd594293be6c51ca",
            "82e94c71469346e9af6a739503377d85",
            "5c890c18837f40748482ac79f472e842",
            "35d2dafc5d5d4d4eade08b9674ad668c",
            "c1bfccfd8002495c9011e71a8cf51d2f",
            "66243573953f4b43bc05914ff693319f",
            "f66ead7e7cdc44418a8873e6f42d35a8",
            "fa1b4910f2d843baabb3af8bab2392a8",
            "8fced898423e4ca290680749c2559703",
            "b98d42b76b8f46bb9c690b170b29c960",
            "0fd9b66b9e0a463ea61fef3094988f32",
            "e525ed1f163c476e80ab09c4f8d2f230",
            "065bb6fe03414729912bd938831175d2",
            "de1bceac10f74089aa21c468d6fb6cbc",
            "05b26fe71b56429bb89e91244b0f6329",
            "fe092da7f4a04a67a0bae3c71a6ac3c8",
            "75fee4e1d92842c383ef7769abb7fc1d",
            "f572dcb54fc54c3eac4a2c047c0a6255",
            "36dead6f206c470fb8028ed8e3f53623"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/drive/.shortcut-targets-by-id/1IS7xxoH06-zPLbTk07CAGSmtMTFE85h_/Deep_Learning_Project/training/LSTM/checkpoints/lstm_seq2seq_checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name            | Type               | Params\n",
            "-------------------------------------------------------\n",
            "0 | token_embedding | Embedding          | 7.4 K \n",
            "1 | LSTM_encoder    | LSTM_pytorch       | 264 K \n",
            "2 | LSTM_decoder    | LSTM_pytorch       | 329 K \n",
            "3 | to_logits       | Sequential         | 24.0 K\n",
            "4 | train_accuracy  | MulticlassAccuracy | 0     \n",
            "5 | val_accuracy    | MulticlassAccuracy | 0     \n",
            "6 | test_accuracy   | MulticlassAccuracy | 0     \n",
            "-------------------------------------------------------\n",
            "625 K     Trainable params\n",
            "0         Non-trainable params\n",
            "625 K     Total params\n",
            "2.501     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "747b77f860224de39a44cdb646710024"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fced898423e4ca290680749c2559703"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRU Seq2Seq from scratch (Not used)"
      ],
      "metadata": {
        "id": "QVGB7UiiMB6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "da fare:\n",
        "\n",
        "-  controllare l'architettura GRU (FATTO)\n",
        "\n",
        "-  controllare teacher forcing (FATTO)\n",
        "\n",
        "\n",
        "\n",
        "-  utilizzare stage (parametro di setup) per caricare anche un solo dataset se stage = \"train\" ad esempio \n",
        "   (https://colab.research.google.com/drive/1oJrA-Q-neOl1fCQJhIWR_GmxpYaG-cFx?authuser=1#scrollTo=JM57yq7bJS0E)\n",
        "\n",
        "-  aggiungere predict_step nel pl.LightningModule dove si chiama inference e relativo predict dataloader nel Lightning data module\n",
        "\n",
        "\n",
        "-  RNN fatte molto bene:\n",
        "    https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch \n",
        "    https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n"
      ],
      "metadata": {
        "id": "KkLLQqqQMB6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Wx = nn.Linear(input_size, 3*hidden_size, bias=True)\n",
        "        self.Wh_gate = nn.Linear(hidden_size, 2*hidden_size, bias=True)\n",
        "\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size, bias = True)\n",
        "\n",
        "        \n",
        "        #nell'implementazione del git c'è anche una funzione reset_parameters\n",
        "\n",
        "    def forward(self, x, h):\n",
        "\n",
        "        x_reset, x_update, x_candidate = torch.tensor_split(self.Wx(x), 3, dim=-1)\n",
        "        \"\"\"\n",
        "        print(f\"shape h {h.shape}\")\n",
        "        print(f\"shape {x.shape}\")\n",
        "        print(f\"hidden_size {self.hidden_size}\")\n",
        "        print(f\"input_size {self.input_size}\")\n",
        "        \"\"\"\n",
        "        #h = h.clone()           #INSERITO\n",
        "\n",
        "        h_reset, h_update = torch.tensor_split(self.Wh_gate(h), 2, dim=-1)\n",
        "\n",
        "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
        "\n",
        "        update_gate = torch.sigmoid(x_update + h_update)\n",
        "\n",
        "        h_candidate = torch.tanh(x_candidate + self.Whh(reset_gate * h))   \n",
        "\n",
        "        h_t = update_gate * h + (1-update_gate) * h_candidate\n",
        "\n",
        "        return h_t\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VtfWqeUlMB6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \n",
        "    def __init__ (self, input_size, hidden_size, num_cells=2, device=None):\n",
        "        assert num_cells>0\n",
        "\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.num_cells = num_cells\n",
        "        \n",
        "\n",
        "        self.GRU_cells = nn.ModuleList(\n",
        "            [GRUCell(input_size, hidden_size)]+[GRUCell(hidden_size, hidden_size) for _ in range(1, num_cells)])\n",
        "        \n",
        "    def forward(self, x, h=None):\n",
        "        \"x è una sequenza [batch, seq_len, embedding_dim]\"\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        \"h [self.num_cells, batch_size, self.hidden_size]\"\n",
        "        #output_states = torch.stack([torch.zeros(batch_size, self.hidden_size) for _ in range(seq_len)], dim = 0)\n",
        "        output_states = torch.zeros((batch_size, seq_len, self.hidden_size), device=x.device)\n",
        "\n",
        "        #print(self.device)\n",
        "        #print(f\"output_states device {output_states.device}\")\n",
        "\n",
        "        if(h!=None):\n",
        "            hidden_states = h\n",
        "        else:\n",
        "            #hidden_states = torch.zeros((self.num_cells, batch_size, self.hidden_size), device=x.device)\n",
        "            hidden_states = [torch.zeros((batch_size, self.hidden_size), device=x.device) for _ in range(self.num_cells)]\n",
        "\n",
        "        #print(f\"hidden_states device {hidden_states.device}\")\n",
        "\n",
        "        \n",
        "        #hidden_states = [torch.zeros(batch_size, self.hidden_size) for _ in range(self.num_cells)]\n",
        "\n",
        "        #hidden_states = [torch.zeros(batch_size, self.hidden_size) for _ in range(self.num_cells)]\n",
        "        \n",
        "        for t in range(seq_len):\n",
        "\n",
        "            x_t = x[:,t,:]\n",
        "\n",
        "            #hidden_states = hidden_states.clone()\n",
        "\n",
        "            hidden_states[0] = self.GRU_cells[0](x_t, hidden_states[0])\n",
        "\n",
        "            for l in range(1, self.num_cells):\n",
        "                #hidden_states = hidden_states.clone()\n",
        "\n",
        "                hidden_states[l] = self.GRU_cells[l](hidden_states[l-1], hidden_states[l])\n",
        "\n",
        "            #output_states = output_states.clone()\n",
        "\n",
        "            output_states[:,t,:] = hidden_states[self.num_cells - 1]\n",
        "\n",
        "        #output_state : [batch_size, seq_len, hidden_size]\n",
        "        return output_states, hidden_states #(così si prende output_states[:,-1,:] da dare al decoder e tutto output_states per il linear dopo il decoder)\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "QtjxQD_bMB6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" prove\n",
        "t = torch.cat([torch.zeros(1, 2, 3) for _ in range(3)], dim = 0)\n",
        "print(t.shape)\n",
        "t[1] = torch.ones(2,3)\n",
        "print(t)\n",
        "t[-1].shape\n",
        "\n",
        "#equivalente a \n",
        "\n",
        "t = torch.stack([torch.zeros(2, 3) for _ in range(3)], dim = 0)\n",
        "print(t.shape)\n",
        "t[1] = torch.ones(2,3)\n",
        "print(t)\n",
        "t[-1].shape\n",
        "\"\"\""
      ],
      "metadata": {
        "outputId": "a8a5980c-210a-4300-9b89-6a38dd1a2fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_BA0eHw5MB6O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' prove\\nt = torch.cat([torch.zeros(1, 2, 3) for _ in range(3)], dim = 0)\\nprint(t.shape)\\nt[1] = torch.ones(2,3)\\nprint(t)\\nt[-1].shape\\n\\n#equivalente a \\n\\nt = torch.stack([torch.zeros(2, 3) for _ in range(3)], dim = 0)\\nprint(t.shape)\\nt[1] = torch.ones(2,3)\\nprint(t)\\nt[-1].shape\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUEncoderDecoder(pl.LightningModule): #oppure Seq2Seq (informiamoci sui nomi)\n",
        "    def __init__(self,\n",
        "        special_idxs: Dict[str, int],\n",
        "        optimizer_params: dict,\n",
        "        learning_rate: float=1e-4,\n",
        "        embedding_dim: float=256,\n",
        "        hidden_size: int=512,\n",
        "        vocabulary_size: int=58,\n",
        "        max_len_question: int=162,\n",
        "        max_len_answer: int=32,\n",
        "        num_cells: int=2,\n",
        "        dropout: float=0.2,\n",
        "    ):\n",
        "        super(GRUEncoderDecoder, self).__init__()\n",
        "\n",
        "        #CONTROLLARE LE DIMENSIONI\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.bos_id = special_idxs['<bos>']\n",
        "        self.eos_id = special_idxs['<eos>']\n",
        "        self.pad_id = special_idxs['<pad>']\n",
        "\n",
        "        self.optimizer_params = optimizer_params\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocabulary_size, embedding_dim, padding_idx = self.pad_id)\n",
        "\n",
        "        self.GRU_encoder = GRU(embedding_dim, hidden_size, num_cells) \n",
        "\n",
        "        self.GRU_decoder = GRU(embedding_dim + hidden_size, hidden_size, num_cells)\n",
        "\n",
        "        self.to_logits = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
        "                                       nn.ReLU(), \n",
        "                                       nn.Dropout(dropout),\n",
        "                                       nn.Linear(hidden_size, vocabulary_size)) \n",
        "        \n",
        "        self.max_len_question = max_len_question\n",
        "        self.max_len_answer = max_len_answer\n",
        "\n",
        "        self.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "        self.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "        self.test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index=self.pad_id)\n",
        "\n",
        "\n",
        "        \n",
        "        #embedding\n",
        "    #encoder GRU\n",
        "    #decoder GRU\n",
        "    #ff per classification\n",
        "    #decoder dovrebbe poter utilizzare teacher forcing credo -> metodo inference come Transformer\n",
        "\n",
        "    def inference(self, x):\n",
        "\n",
        "        #encode and then generate the output token by token greedily\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_size = x.size(0)\n",
        "            x = self.token_embedding(x)\n",
        "\n",
        "            #print(f\"x device {x.device}\")\n",
        "\n",
        "            output_encoder, previous_state = self.GRU_encoder(x)\n",
        "\n",
        "            #print(f\"output_encoder device {output_encoder.device}\")\n",
        "            #print(f\"previous_state device {output_encoder.device}\")\n",
        "\n",
        "\n",
        "            last_state_encoder = previous_state[-1].unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "            output = torch.ones(batch_size, 1, dtype=torch.int64, device = self.device).fill_(self.bos_id)\n",
        "            done = torch.zeros(batch_size, dtype = torch.uint8, device = self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            for _ in range(self.max_len_answer - 1):\n",
        "                #last_state_encoder_repeated = last_state_encoder.repeat(current_output.shape[1],1,1).transpose(0,1) \n",
        "\n",
        "                    #output.shape[1] è 1, non è un problema visto che nel decoder consideriamo sempre un token\n",
        "\n",
        "                current_output = output[:,-1].unsqueeze(dim=1)\n",
        "\n",
        "                current_output_embedding = self.token_embedding(current_output)\n",
        "\n",
        "                input_decoder = torch.cat((last_state_encoder, current_output_embedding), dim=-1)\n",
        "\n",
        "                out, previous_state = self.GRU_decoder(input_decoder, previous_state)\n",
        "\n",
        "                out = self.to_logits(out)\n",
        "\n",
        "                out = torch.argmax(out[:,[-1],:], dim = -1)\n",
        "\n",
        "                output = torch.cat([output, out], dim = 1)\n",
        "\n",
        "                eos_reached = out.squeeze(1) == self.eos_id\n",
        "                done |= eos_reached\n",
        "                if done.sum() == batch_size:\n",
        "                    break\n",
        "\n",
        "            return output\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        x = self.token_embedding(x)\n",
        "\n",
        "        y = self.token_embedding(y)\n",
        "\n",
        "        #print(f\"shape x {x.shape}\")\n",
        "        #print(f\"shape y {y.shape}\")\n",
        "\n",
        "\n",
        "        output_encoder, state_encoder = self.GRU_encoder(x) \n",
        "\n",
        "        #print(f\"shape output_encoder {output_encoder.shape}\")\n",
        "        #print(f\"shape state_encoder {state_encoder.shape}\")\n",
        "\n",
        "        last_state_encoder = state_encoder[-1]\n",
        "\n",
        "        #print(f\"shape last_state_encoder {last_state_encoder.shape}\")\n",
        "        #state_encoder = self.GRU_encoder(x)[:,-1,:]\n",
        "\n",
        "        last_state_encoder_repeated = last_state_encoder.repeat(y.shape[1],1,1).transpose(0,1) #CONTROLLA\n",
        "\n",
        "        #print(f\"shape last_state_encoder_repeated {last_state_encoder_repeated.shape}\")\n",
        "\n",
        "        input_decoder = torch.cat((last_state_encoder_repeated, y), dim=-1)\n",
        "\n",
        "        #print(f\"shape input_decoder {input_decoder.shape}\")\n",
        "        output_decoder, _ = self.GRU_decoder(input_decoder, state_encoder)\n",
        "\n",
        "        #print(f\"shape output_decoder {output_decoder.shape}\")\n",
        "        return self.to_logits(output_decoder).transpose(1,2)\n",
        "        \n",
        "    \n",
        "    def configure_optimizers(self):# learning rate = 1x10^-4; beta1 =0.9; beta2 = 0.995 dal paper\n",
        "        betas = self.optimizer_params['betas']\n",
        "        return torch.optim.Adam(self.parameters(), self.learning_rate, betas)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self(x, y)\n",
        "        loss = F.cross_entropy(y_pred, y, ignore_index = self.pad_id)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        self.train_accuracy.update(y_pred, y)\n",
        "        self.log('train_accuracy_epoch', self.train_accuracy.compute(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x)  #[batch_size, max_eos_found]\n",
        "        y_pred = F.pad(y_pred, (0, self.max_len_answer - y_pred.shape[1]), mode='constant', value=self.pad_id) #[batch_size, max_len_answer]\n",
        "        self.val_accuracy.update(y_pred, y)\n",
        "        self.log('val_accuracy_step', self.val_accuracy.compute(), on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x)  #[batch_size, max_eos_found]\n",
        "        y_pred = F.pad(y_pred, (0, self.max_len_answer - y_pred.shape[1]), mode='constant', value=self.pad_id) #[batch_size, max_len_answer]\n",
        "        self.test_accuracy.update(y_pred, y)\n",
        "        \n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.log('val_accuracy_epoch', self.val_accuracy.compute(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.val_accuracy.reset()\n",
        "        \n",
        "        # Also reset the training accuracy\n",
        "        self.train_accuracy.reset()\n",
        "\n",
        "    \n",
        "    def test_epoch_end(self, outputs):\n",
        "        self.log('test_accuracy_epoch', self.test_accuracy.compute(), on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.test_accuracy.reset()\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "zB2OWi5uMB6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDCWfYaeMEyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}