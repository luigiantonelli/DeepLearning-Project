{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiantonelli/DeepLearning-Project/blob/main/Deep_Learning_Project_Antonelli_Cuconasu_Gaudenzi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports"
      ],
      "metadata": {
        "id": "_0HXoR9RpcO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxGf0hOgnsPD"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle as pkl\n",
        "from tqdm.notebook import tqdm\n",
        "import pytorch_lightning as pl "
      ],
      "metadata": {
        "id": "DobcczcWqek-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "P4AfeDpYq3dL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOKF8rS40H5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "ho9Tk3GctrvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#forse meglio definire una stable softmax"
      ],
      "metadata": {
        "id": "tg6p4Y4ZXAyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product_attention(query, key, value, sqrt_q, device, mask = None):\n",
        "    t = torch.matmul(query, key.transpose(-2, -1))/sqrt_q\n",
        "    if mask is not None:\n",
        "      t = t.masked_fill_(mask == 0, -1e-10) #-1e-10 acts like -infinity, so that the softmax will consider these tokens less important\n",
        "    return torch.matmul(F.softmax(t, dim = -1), value)"
      ],
      "metadata": {
        "id": "9JbWbeOJtt02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d, num_heads, batch_size):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d % h == 0\n",
        "    self.dim_head = d // num_heads #single head dimension\n",
        "    self.sqrt_q = sqrt(self.dim_head)\n",
        "    self.num_heads = num_heads\n",
        "    self.batch_size = batch_size\n",
        "    self.W_q = nn.Linear(d, d, bias = False) #stack of num_heads matrices of dimension (d, dim_head), one for each head\n",
        "    self.W_k = nn.Linear(d, d, bias = False)\n",
        "    self.W_v = nn.Linear(d, d, bias = False)\n",
        "    self.W_o = nn.Linear(d, d, bias = False)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None): #query, key, value\n",
        "    query = self.W_q(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "    key = self.W_k(key).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "    value = self.W_v(value).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "    attention_value = dot_product_attention(query, key, value, self.sqrt_q, mask)\n",
        "    return self.W_o(attention_value.transpose(1, 2).contiguous().view(self.batch_size, -1, self.num_heads*self.dim_head))"
      ],
      "metadata": {
        "id": "cdtptdthS6Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TP_MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d, num_heads, batch_size):\n",
        "    super(TP_MultiHeadAttention, self).__init__()\n",
        "    assert d % h == 0\n",
        "    self.dim_head = d // num_heads #single head dimension\n",
        "    self.sqrt_q = sqrt(self.dim_head)\n",
        "    self.num_heads = num_heads\n",
        "    self.batch_size = batch_size\n",
        "    self.W_q = nn.Linear(d, d, bias = True) #stack of num_heads matrices of dimension (d, dim_head), one for each head\n",
        "    self.W_k = nn.Linear(d, d, bias = True)\n",
        "    self.W_v = nn.Linear(d, d, bias = True)\n",
        "    self.W_o = nn.Linear(d, d, bias = True)\n",
        "    self.W_r = nn.Linear(d, d, bias = True) #ruolo\n",
        "\n",
        "  def forward(self, query, key, value, mask = None): #query, key, value\n",
        "    pass"
      ],
      "metadata": {
        "id": "lEUjRO5EURve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d, num_heads, batch_size, hidden_size, dropout, tp_attention = False):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.d = d\n",
        "    self.num_heads = num_heads\n",
        "    self.batch_size = batch_size\n",
        "    self.attention = MultiHeadAttention(d, h, batch_size) if not tp_attention else TP_MultiHeadAttention(d, h, batch_size)\n",
        "    self.norm1 = nn.LayerNorm(d)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.norm2 = nn.LayerNorm(d)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.ff = nn.Sequential(nn.Linear(d, hidden_size, bias = True), \n",
        "                            nn.ReLU(inplace = True),\n",
        "                            nn.Linear(hidden_size, d, bias = True))\n",
        "  def forward(self, query, key, value, mask): #query, key, value\n",
        "    x = value + self.attention(query, key, value, mask)\n",
        "    x = self.dropout1(self.norm1(x))\n",
        "    x = x + self.ff(x)\n",
        "    x = self.dropout2(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "GhrmQH8sUHwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d, h, batch_size, hidden_size, dropout, tp_attention = False):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(d, h, batch_size) if not tp_attention else TP_MultiHeadAttention(d, h, batch_size)\n",
        "    self.norm = nn.LayerNorm(d)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.transformer_block = TransformerBlock(d, h, batch_size, hidden_size, dropout, tp_attention)\n",
        "\n",
        "  def forward(self, query, key, value, mask): #serve output encoder\n",
        "    pass"
      ],
      "metadata": {
        "id": "w0MIvv-ZWL-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d, max_len = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    pe = torch.zeros(max_len, d)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d, 2) * -(math.log(10000.0) / d))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return x + Variable(self.pe[:, :x.size(1)], requires_grad = False)"
      ],
      "metadata": {
        "id": "qrsDFyoyUF0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, d, num_heads, batch_size, hidden_size, dropout, num_blocks = 6, tp_attention = False):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d = d\n",
        "    self.num_heads = num_heads\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder = nn.ModuleList(\n",
        "        [TransformerBlock(d, num_heads, batch_size, hidden_size, dropout, tp_attention) for _ in range(num_blocks)]\n",
        "        )\n",
        "    \n",
        "  def forward(self, x): \n",
        "    #self.encoder(x)\n",
        "    pass"
      ],
      "metadata": {
        "id": "bgNDRuG5YIWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, d, num_heads, batch_size, hidden_size, dropout, num_blocks = 6, tp_attention = False):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d = d\n",
        "    self.num_heads = num_heads\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder = nn.ModuleList(\n",
        "        [DecoderBlock(d, num_heads, batch_size, hidden_size, dropout, tp_attention) for _ in range(num_blocks)]\n",
        "        )\n",
        "    \n",
        "  def forward(self, output_encoder, x): \n",
        "    pass"
      ],
      "metadata": {
        "id": "Aybtz4uUY8vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d, num_heads, batch_size, hidden_size, dropout, num_blocks_encoder = 6, num_blocks_decoder = 6, tp_attention = False):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d = d\n",
        "    self.num_heads = num_heads\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder = TransformerEncoder(d, num_heads, batch_size, hidden_size, dropout, num_blocks_encoder, tp_attention)\n",
        "    self.decoder = TransformerDecoder(d, num_heads, batch_size, hidden_size, dropout, num_blocks_decoder, tp_attention)\n",
        "\n",
        "  def inference(self, x):\n",
        "    #encode and then generate the output token by token\n",
        "    pass\n",
        "    \n",
        "  def forward(self, x): \n",
        "    pass"
      ],
      "metadata": {
        "id": "qiPC8tqNY8wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOTA"
      ],
      "metadata": {
        "id": "sEka3B3Eq69g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uf3QJeF-q-AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NON-SOTA"
      ],
      "metadata": {
        "id": "0b4LEKhjtizS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mm-mxAgbtmPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}