{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2646c60c45914877900b986114afd422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f78aa206abd45f78e656e04afcf68a7",
              "IPY_MODEL_a60ad952986f4a84aeff3b67cbb32b18",
              "IPY_MODEL_ec41302ef3104f9196a31970b43989bb"
            ],
            "layout": "IPY_MODEL_499734b415fd4fd8854e216be9e700ee"
          }
        },
        "1f78aa206abd45f78e656e04afcf68a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdbc0c96e46d4ae7bbca96ef5827b36f",
            "placeholder": "​",
            "style": "IPY_MODEL_5e001073360d4b9293c38ab48ba4ccd1",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "a60ad952986f4a84aeff3b67cbb32b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd91fde42bbb433cbf48110ef2342cf3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcbfa75e054a412f988ac462da699c5a",
            "value": 2
          }
        },
        "ec41302ef3104f9196a31970b43989bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676c772548554b7ca2cb611917340758",
            "placeholder": "​",
            "style": "IPY_MODEL_ac876b196f3d443f81d364c2598b25d0",
            "value": " 2/2 [00:03&lt;00:00,  1.57s/it]"
          }
        },
        "499734b415fd4fd8854e216be9e700ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "cdbc0c96e46d4ae7bbca96ef5827b36f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e001073360d4b9293c38ab48ba4ccd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd91fde42bbb433cbf48110ef2342cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcbfa75e054a412f988ac462da699c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "676c772548554b7ca2cb611917340758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac876b196f3d443f81d364c2598b25d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df57e6630ae4425aae228aba0865e583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74c3c41f003e4836943a48aac8931852",
              "IPY_MODEL_555fb76b98384709a048f8375615253d",
              "IPY_MODEL_dae36f5062964b758374abc33d6f4fbe"
            ],
            "layout": "IPY_MODEL_debcd5fb2fe94d9db9e035427a02dd17"
          }
        },
        "74c3c41f003e4836943a48aac8931852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7639369125e4856adf918cfe7265749",
            "placeholder": "​",
            "style": "IPY_MODEL_a044b238d1be4f578116a360d59d96c0",
            "value": "Epoch 0:   1%"
          }
        },
        "555fb76b98384709a048f8375615253d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34117cb9bf1643e48144def77da11015",
            "max": 16667,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e277f34cc6e440a3aa4af55bdde40db4",
            "value": 140
          }
        },
        "dae36f5062964b758374abc33d6f4fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ee5226ba0c42159b131242709de363",
            "placeholder": "​",
            "style": "IPY_MODEL_1b5aea2e659e44bb85fdb0860bf4187d",
            "value": " 140/16667 [00:18&lt;36:45,  7.49it/s, loss=0.167, v_num=6, train_loss_step=0.112]"
          }
        },
        "debcd5fb2fe94d9db9e035427a02dd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c7639369125e4856adf918cfe7265749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a044b238d1be4f578116a360d59d96c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34117cb9bf1643e48144def77da11015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e277f34cc6e440a3aa4af55bdde40db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25ee5226ba0c42159b131242709de363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5aea2e659e44bb85fdb0860bf4187d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiantonelli/DeepLearning-Project/blob/main/Deep_Learning_Project_Antonelli_Cuconasu_Gaudenzi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports"
      ],
      "metadata": {
        "id": "_0HXoR9RpcO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxGf0hOgnsPD"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install torchmetrics --quiet\n",
        "!pip install gdown==4.5.4 --no-cache-dir --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import pickle as pkl\n",
        "from tqdm.notebook import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "import math\n",
        "from math import sqrt\n",
        "import pickle\n",
        "from typing import *\n",
        "import gdown"
      ],
      "metadata": {
        "id": "DobcczcWqek-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "url = \"https://drive.google.com/drive/folders/1-6MRkFoSSRJqeKgcMXm3PeA159KzHuB_?usp=sharing\"\n",
        "gdown.download_folder(url = url, quiet = True, use_cookies = False, remaining_ok=True)\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVNGXDrHt71Y",
        "outputId": "100e4578-3bce-49f3-adf2-cdb1998d74d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder_path = \"/content/drive/MyDrive/Colab Notebooks/Deep Learning/DeepLearningProject-Shared\"\n",
        "# dataset_folder_path = \"/content/drive/MyDrive/Deep_Learning_Project\"\n",
        "os.chdir(dataset_folder_path)"
      ],
      "metadata": {
        "id": "gDIk7qc1uP4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "89bxAqHuiCnI",
        "outputId": "ff695627-9eca-4154-e44e-224db64367b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets  lightning_logs  mathematics_dataset-v1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ],
      "metadata": {
        "id": "k6elSDnfiEcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we analyzed all the dataset files to retrieve the characters that will compose the vocabulary. Indeed, we wanted to be sure that our vocabulary contains all the files characters regardless the module we are working on.\n",
        "\n",
        "Moreover, after this pre-processing phase we decided to add the special token `<unk>` (i.e., unknown). Thus, if during inference we are using characters that are not in the vocabulary, we are still able to pre-processes the input, since whathever unknown character is replaced by that special token.  "
      ],
      "metadata": {
        "id": "dK9a2Co1iFIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(text_path: str, lowercase: bool=True) -> Tuple[List[str], List[str]]:\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    with open(text_path) as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if lowercase:\n",
        "                if idx % 2 == 0: # Questions\n",
        "                    questions.append(line.rstrip().lower()) \n",
        "                else: # Answers\n",
        "                    answers.append(line.rstrip().lower())\n",
        "\n",
        "    return questions, answers"
      ],
      "metadata": {
        "id": "x76zoSD5iIAl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(lists_of_texts: List[List[str]]) -> Set[str]:\n",
        "    unified_text = []\n",
        "    \n",
        "    for l in lists_of_texts:\n",
        "        unified_text += l\n",
        "\n",
        "    return Counter(\" \".join(unified_text)).keys()"
      ],
      "metadata": {
        "id": "j8TdNTvhiGgq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all files\n",
        "folders = ['extrapolate', 'interpolate', 'train-easy', 'train-medium', 'train-hard']\n",
        "files = []\n",
        "\n",
        "for fold in folders:\n",
        "    files += glob.glob(f\"./mathematics_dataset-v1.0/{fold}/*.txt\")"
      ],
      "metadata": {
        "id": "cq9xWuA1iJRu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files[:5]"
      ],
      "metadata": {
        "id": "MYRnaDRoiKne",
        "outputId": "972c5c45-2542-4b34-ec2c-80c0ef289fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./mathematics_dataset-v1.0/extrapolate/arithmetic__div_big.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/arithmetic__add_sub_multiple_longer.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/algebra__polynomial_roots_big.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/arithmetic__add_or_sub_big.txt',\n",
              " './mathematics_dataset-v1.0/extrapolate/comparison__kth_biggest_more.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files_vocabulary(files: List[str], save: bool=False) -> List[str]:\n",
        "    vocabulary = []\n",
        "    all_lists = []\n",
        "\n",
        "    i = 0\n",
        "    for f in files:\n",
        "        train, test = read_dataset(f)\n",
        "        all_lists += train\n",
        "        all_lists += test\n",
        "        \n",
        "        # Set union\n",
        "        vocabulary += get_vocabulary(all_lists)\n",
        "        all_lists = []\n",
        "\n",
        "        # Save the vocabulary up to now\n",
        "        if save and i % 10 == 0:\n",
        "            with open('./datasets/pre_vocabulary.pkl', 'wb') as f:\n",
        "                pickle.dump(vocabulary, f)\n",
        "\n",
        "    # Save sorted vocabulary\n",
        "    vocabulary = sorted(list(vocabulary))\n",
        "    with open('./datasets/pre_vocabulary.pkl', 'wb') as f:\n",
        "        pickle.dump(vocabulary, f)\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "VPF92pLiiLxv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This operation requires quite a bit of time (~ 25 min), as we are scanning all the files. So, it is commented to avoid executing it.\n",
        "\n",
        "    vocabulary = get_files_vocabulary(files)"
      ],
      "metadata": {
        "id": "OgEU4A67iNWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary_from_set(voc):\n",
        "    vocabulary = {'<bos>': 0, '<eos>': 1, '<unk>': 2, '<pad>': 3}\n",
        "    i = 4\n",
        "    for v in voc:\n",
        "        vocabulary[v] = i\n",
        "        i += 1\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "BB9BrPMCiOxz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./datasets/pre_vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)"
      ],
      "metadata": {
        "id": "BEOK2TkSiQ_8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulary)"
      ],
      "metadata": {
        "id": "4qBLpmmOiR45",
        "outputId": "05cf3416-a197-4506-ba02-2626c855b7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = create_vocabulary_from_set(vocabulary)"
      ],
      "metadata": {
        "id": "RXBLRw_IiSr9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(v)"
      ],
      "metadata": {
        "id": "ePV3r_4AiThl",
        "outputId": "a5f231ef-29e7-4e4e-ecb8-1f937161c82a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "jSp1SqYRiYkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_module_files(module_name: str) -> List[str]:\n",
        "    folders = ['train-easy', 'train-medium', 'train-hard']\n",
        "    files = []\n",
        "\n",
        "    for fold in folders:\n",
        "        files += glob.glob(f\"./mathematics_dataset-v1.0/{fold}/{module_name}.txt\")\n",
        "\n",
        "    return files"
      ],
      "metadata": {
        "id": "mM60pprQiaQ-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module_files = get_all_module_files(\"algebra__linear_1d\")\n",
        "module_files"
      ],
      "metadata": {
        "id": "uMYdwf5PibSx",
        "outputId": "29a1a1f3-4049-4ead-fe3a-84978733e8a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt',\n",
              " './mathematics_dataset-v1.0/train-medium/algebra__linear_1d.txt',\n",
              " './mathematics_dataset-v1.0/train-hard/algebra__linear_1d.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_all_module_files(module_name: str) -> Tuple[List[str], List[str]]:\n",
        "    module_files = get_all_module_files(module_name)\n",
        "    module_train_lists = []\n",
        "    module_test_lists = []\n",
        "\n",
        "    for f in module_files:\n",
        "        train, test = read_dataset(f)\n",
        "        module_train_lists += train\n",
        "        module_test_lists += test\n",
        "\n",
        "    return module_train_lists, module_test_lists"
      ],
      "metadata": {
        "id": "Jxr8UcdfidO6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# algebra_train, algebra_test = read_all_module_files(\"algebra__linear_1d\")"
      ],
      "metadata": {
        "id": "M3RGCojMidzD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(algebra_train)"
      ],
      "metadata": {
        "id": "pgi-i53Piezk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algebra_path = \"./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt\"\n",
        "probability_path = \"./mathematics_dataset-v1.0/train-easy/probability__swr_p_level_set.txt\"\n",
        "prime_path = \"./mathematics_dataset-v1.0/train-easy/numbers__is_prime.txt\""
      ],
      "metadata": {
        "id": "9YnrTBHsigmg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "questions_easy_algebra, answers_easy_algebra = read_dataset(algebra_path)\n",
        "questions_easy_probability, answers_easy_probability = read_dataset(probability_path)\n",
        "questions_easy_prime, answers_easy_prime = read_dataset(prime_path)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EzvNILPFigiI",
        "outputId": "fbed9571-cd41-42b5-be20-a62ac10f363e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nquestions_easy_algebra, answers_easy_algebra = read_dataset(algebra_path)\\nquestions_easy_probability, answers_easy_probability = read_dataset(probability_path)\\nquestions_easy_prime, answers_easy_prime = read_dataset(prime_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mathematics_Dataset(Dataset):\n",
        "    def __init__(self, modules: List[str], vocabulary: dict):\n",
        "        self.modules = modules\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        for m in self.modules:\n",
        "            q_m, a_m = self.read_dataset(m)\n",
        "            self.questions += q_m\n",
        "            self.answers += a_m\n",
        "        self.max_len_question = 160 #forse serve multiplo di num_heads\n",
        "        self.max_len_answer = 30\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "    def read_dataset(self, text_path: str, lowercase: bool=True) -> Tuple[List[str], List[str]]:\n",
        "        questions = []\n",
        "        answers = []\n",
        "        with open(text_path, 'r') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                if lowercase:\n",
        "                    if idx % 2 == 0: # Questions\n",
        "                        questions.append(line.rstrip().lower()) \n",
        "                    else: # Answers\n",
        "                        answers.append(line.rstrip().lower())\n",
        "        return questions, answers\n",
        "\n",
        "    def convert_chars_to_ids(self, sentence: str, max_len: int) -> torch.tensor:\n",
        "        sentence_ids = np.full(max_len + 2, self.vocabulary['<pad>'])\n",
        "\n",
        "        # Start with <bos>\n",
        "        sentence_ids[0] = self.vocabulary['<bos>']\n",
        "\n",
        "        for i, char in enumerate(sentence):\n",
        "            sentence_ids[i + 1] = self.vocabulary.get(char, self.vocabulary['<unk>'])\n",
        "            \n",
        "        # End with <eos>\n",
        "        sentence_ids[len(sentence) + 1] = self.vocabulary['<eos>']\n",
        "\n",
        "        return torch.from_numpy(sentence_ids).long()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.questions)\n",
        "        \n",
        "        q, a = self.questions[idx], self.answers[idx]\n",
        "\n",
        "        question = self.convert_chars_to_ids(q, self.max_len_question)\n",
        "        answer = self.convert_chars_to_ids(a, self.max_len_answer)\n",
        "        \n",
        "        return question, answer"
      ],
      "metadata": {
        "id": "hx1639OhiiDQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = Mathematics_Dataset(['./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt'], v)"
      ],
      "metadata": {
        "id": "YDOrfk6diob6"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(d)"
      ],
      "metadata": {
        "id": "8AblWWL6ipsG",
        "outputId": "4e5a3190-7a39-4fb1-b630-2008c5029abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "666666"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q, a = d[8]"
      ],
      "metadata": {
        "id": "jxC4cnLeiqW4"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q, a"
      ],
      "metadata": {
        "id": "10CuO-uHirB2",
        "outputId": "1e7c3f96-9893-4a1b-b10f-1d3b5774a79a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0, 48, 44, 41, 51, 34,  4, 12, 16, 20,  9, 35,  4, 10,  4, 17, 16,  9,\n",
              "         35,  4, 12,  4, 16, 17,  4, 27,  4, 15,  4, 35, 44, 47,  4, 35, 13,  1,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]),\n",
              " tensor([ 0, 17,  1,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mathematics_DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, modules: List[str], batch_size: int = 32):\n",
        "        super().__init__()\n",
        "        self.modules = modules\n",
        "        self.batch_size = batch_size\n",
        "        self.load_vocabulary()\n",
        "    \n",
        "    def load_vocabulary(self):\n",
        "        with open('./datasets/vocabulary.pkl', 'rb') as f:\n",
        "            v = pickle.load(f)\n",
        "        self.vocabulary = create_vocabulary_from_set(v)\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        self.math = Mathematics_Dataset(self.modules, self.vocabulary)\n",
        "        self.math_train, self.math_val, self.math_test = random_split(self.math, [0.75, 0.05, 0.20])\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.math_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):                                                              \n",
        "        return DataLoader(self.math_val, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.math_test, batch_size=self.batch_size)\n",
        "\n",
        "    def teardown(self, stage: str):\n",
        "        # Used to clean-up when the run is finished\n",
        "        pass"
      ],
      "metadata": {
        "id": "QVBvNs1QisOI"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm = Mathematics_DataModule(['./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt'], batch_size = 64)"
      ],
      "metadata": {
        "id": "nPg1DDjritNc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "ho9Tk3GctrvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#forse meglio definire una stable softmax\n",
        "\"\"\"\n",
        "la x emb_d @ emb_d x lq\n",
        "la x lq (forse errore perché la maschera è la x la)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tg6p4Y4ZXAyU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a825154d-7f96-4974-d92c-342c2de51298"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nla x emb_d @ emb_d x lq\\nla x lq (forse errore perché la maschera è la x la)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    sqrt_q = sqrt(query.size(-1))\n",
        "\n",
        "    t = torch.matmul(query, key.transpose(-2, -1)) / sqrt_q\n",
        "    t = t.masked_fill_(mask == 0, -1e-10) #-1e-10 acts like -infinity, so that the softmax will consider these tokens less important\n",
        "    return torch.matmul(F.softmax(t, dim = -1), value)"
      ],
      "metadata": {
        "id": "9JbWbeOJtt02"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module): \n",
        "    def __init__(self, embedding_dim, num_heads, batch_size, tp_attention = False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        self.tp_attention = tp_attention\n",
        "        self.dim_head = embedding_dim // num_heads #single head dimension\n",
        "        # self.sqrt_q = sqrt(self.dim_head)\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.W_q = nn.Linear(embedding_dim, embedding_dim, bias = True) #stack of num_heads matrices of dimension (d, dim_head), one for each head\n",
        "        self.W_k = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        self.W_v = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        self.W_o = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        if self.tp_attention:\n",
        "            self.W_r = nn.Linear(embedding_dim, embedding_dim, bias = True) #ruolo\n",
        "\n",
        "    def forward(self, query, key, value, mask): #query, key, value\n",
        "        q = self.W_q(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        k = self.W_k(key).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        v = self.W_v(value).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "\n",
        "        attention_value = scaled_dot_product_attention(q, k, v, mask)\n",
        "        \n",
        "        if self.tp_attention:\n",
        "            role = self.W_r(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "            attention_value *= role  #element-wise product between attention value and role before the final projection\n",
        "        return self.W_o(attention_value.transpose(1, 2).contiguous().view(self.batch_size, -1, self.num_heads*self.dim_head))"
      ],
      "metadata": {
        "id": "cdtptdthS6Td"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class TP_MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, batch_size):\n",
        "        super(TP_MultiHeadAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        self.dim_head = embedding_dim // num_heads #single head dimension\n",
        "        self.sqrt_q = sqrt(self.dim_head)\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.W_q = nn.Linear(embedding_dim, embedding_dim, bias = True) #stack of num_heads matrices of dimension (d, dim_head), one for each head\n",
        "        self.W_k = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        self.W_v = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "        self.W_o = nn.Linear(embedding_dim, embedding_dim, bias = True)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None): #query, key, value\n",
        "        q = self.W_q(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        k = self.W_k(key).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        v = self.W_v(value).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        role = self.W_r(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "        attention_value = scaled_dot_product_attention(q, k, v, self.sqrt_q, mask).transpose(1, 2) \n",
        "        return self.W_o((attention_value * role).contiguous().view(self.batch_size, -1, self.num_heads*self.dim_head)) #element-wise product between attention value and role before the final projection\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lEUjRO5EURve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "5e6c135b-c6dd-4786-f1f2-38cf94d8ffa2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass TP_MultiHeadAttention(nn.Module):\\n    def __init__(self, embedding_dim, num_heads, batch_size):\\n        super(TP_MultiHeadAttention, self).__init__()\\n        assert embedding_dim % num_heads == 0\\n        self.dim_head = embedding_dim // num_heads #single head dimension\\n        self.sqrt_q = sqrt(self.dim_head)\\n        self.num_heads = num_heads\\n        self.batch_size = batch_size\\n        self.W_q = nn.Linear(embedding_dim, embedding_dim, bias = True) #stack of num_heads matrices of dimension (d, dim_head), one for each head\\n        self.W_k = nn.Linear(embedding_dim, embedding_dim, bias = True)\\n        self.W_v = nn.Linear(embedding_dim, embedding_dim, bias = True)\\n        self.W_o = nn.Linear(embedding_dim, embedding_dim, bias = True)\\n\\n    def forward(self, query, key, value, mask = None): #query, key, value\\n        q = self.W_q(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\\n        k = self.W_k(key).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\\n        v = self.W_v(value).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\\n        role = self.W_r(query).view(self.batch_size, -1, self.num_heads, self.dim_head).transpose(1, 2)\\n        attention_value = scaled_dot_product_attention(q, k, v, self.sqrt_q, mask).transpose(1, 2) \\n        return self.W_o((attention_value * role).contiguous().view(self.batch_size, -1, self.num_heads*self.dim_head)) #element-wise product between attention value and role before the final projection\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, batch_size, hidden_size = None, dropout=0.2, tp_attention = False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.attention = MultiHeadAttention(embedding_dim, num_heads, batch_size, tp_attention)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        hidden_size = 4*embedding_dim if hidden_size is None else hidden_size\n",
        "        self.ff = nn.Sequential(nn.Linear(embedding_dim, hidden_size, bias = True), \n",
        "                                nn.ReLU(inplace = True),\n",
        "                                nn.Linear(hidden_size, embedding_dim, bias = True))\n",
        "\n",
        "    def forward(self, query, key, value, mask): #query, key, value\n",
        "        x = query + self.attention(query, key, value, mask) #query as res conn because the decoder block requires it and it doesn't matter for encoder blocks\n",
        "        x = self.dropout1(self.norm1(x))\n",
        "        x = x + self.ff(x)\n",
        "        x = self.dropout2(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "GhrmQH8sUHwE"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, batch_size, hidden_size, dropout = 0.2, tp_attention = False):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.masked_attention = MultiHeadAttention(embedding_dim, num_heads, batch_size, tp_attention)\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.transformer_block = TransformerBlock(embedding_dim, num_heads, batch_size, hidden_size, dropout, tp_attention)\n",
        "\n",
        "    def forward(self, output_encoder, src_mask, y, trg_mask):\n",
        "        y = y + self.masked_attention(y, y, y, trg_mask) #masked attention (y = query = key = value) + residual connection\n",
        "        y = self.dropout(self.norm(y))\n",
        "        return self.transformer_block(y, output_encoder, output_encoder, src_mask)#query from the masked mha and key and value from the encoder"
      ],
      "metadata": {
        "id": "w0MIvv-ZWL-M"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + Variable(self.pe[:, :x.size(1)], requires_grad = False)"
      ],
      "metadata": {
        "id": "qrsDFyoyUF0D"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, batch_size, hidden_size, dropout, num_blocks = 6, tp_attention = False):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder = nn.ModuleList(\n",
        "            [TransformerBlock(embedding_dim, num_heads, batch_size, hidden_size, dropout, tp_attention) for _ in range(num_blocks)]\n",
        "            )\n",
        "\n",
        "    def forward(self, x, mask): \n",
        "        for block in self.encoder:\n",
        "            x = block(x, x, x, mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "bgNDRuG5YIWz"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, batch_size, hidden_size, dropout = 0.2, num_blocks = 6, tp_attention = False):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.decoder = nn.ModuleList(\n",
        "            [DecoderBlock(embedding_dim, num_heads, batch_size, hidden_size, dropout, tp_attention) for _ in range(num_blocks)]\n",
        "            )\n",
        "\n",
        "    def forward(self, output_encoder, src_mask, y, trg_mask): \n",
        "        for block in self.decoder:\n",
        "            y = block(output_encoder, src_mask, y, trg_mask)\n",
        "        return y"
      ],
      "metadata": {
        "id": "Aybtz4uUY8vQ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(pl.LightningModule):\n",
        "    def __init__(self, embedding_dim = 256, num_heads = 4, batch_size = 32, hidden_size = 512, dropout = 0.2, vocabulary_size = 58, bos_id = 0, pad_id = 3, num_blocks_encoder = 6, num_blocks_decoder = 6, tp_attention = False):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.bos_id = bos_id\n",
        "        self.pad_id = pad_id\n",
        "        self.token_embedding = nn.Embedding(vocabulary_size, embedding_dim, padding_idx = self.pad_id)\n",
        "        self.positional_embedding = PositionalEncoding(embedding_dim)\n",
        "        self.encoder = TransformerEncoder(embedding_dim, num_heads, batch_size, hidden_size, dropout, num_blocks_encoder, tp_attention)\n",
        "        self.decoder = TransformerDecoder(embedding_dim, num_heads, batch_size, hidden_size, dropout, num_blocks_decoder, tp_attention)\n",
        "        self.to_logits = nn.Linear(embedding_dim, vocabulary_size)\n",
        "        \n",
        "        self.max_len_question = 162\n",
        "        self.max_len_answer = 32\n",
        "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocabulary_size, ignore_index = self.pad_id)\n",
        "\n",
        "    def create_trg_mask(self, y): #compute a mask so that the prediction of the next token can only depend on the previous tokens\n",
        "        # #[batch_size, 1, len, len] & [batch_size, 1, 1, len]\n",
        "        #return torch.logical_and(self.create_causal_mask(y), self.create_padding_mask(y))\n",
        "        return self.create_causal_mask(y) & self.create_padding_mask(y)\n",
        "\n",
        "    def create_causal_mask(self, y):\n",
        "        batch_size, seq_len = y.shape\n",
        "        mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.int64, device = self.device)).expand(\n",
        "            batch_size, 1, seq_len, seq_len)\n",
        "        return mask#.to(y).long()\n",
        "\n",
        "    def create_padding_mask(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        mask = (x != self.pad_id).unsqueeze(-2).unsqueeze(-2).expand(\n",
        "                batch_size, 1, 1, seq_len)\n",
        "        return mask#.to(x).long()\n",
        "\n",
        "    def inference(self, x):\n",
        "        #encode and then generate the output token by token greedily\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            src_mask = self.create_padding_mask(x)\n",
        "            x = self.token_embedding(x)\n",
        "            x = self.positional_embedding(x)\n",
        "            output_encoder = self.encoder(x, src_mask)\n",
        "            output = torch.ones(x.shape[0], 1, dtype=torch.int64, device = self.device).fill_(self.bos_id)#.to(x).long()\n",
        "            \n",
        "            for i in range(self.max_len_answer - 1): \n",
        "                trg_mask = self.create_trg_mask(output)\n",
        "                output_embedding = self.token_embedding(output)\n",
        "                output_embedding = self.positional_embedding(output_embedding)\n",
        "                out = self.decoder(output_encoder, src_mask, output_embedding, trg_mask)\n",
        "                out = self.to_logits(out) #[batch, len, vocab_size] [batch, len-1]\n",
        "                out = torch.argmax(out[:,[-1],:], dim = -1)#.to(x).long()\n",
        "                output = torch.cat([output, out], dim = 1)#.to(x).long()\n",
        "            return output\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        src_mask = self.create_padding_mask(x)\n",
        "        trg_mask = self.create_trg_mask(y)\n",
        "\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.positional_embedding(x)\n",
        "        y = self.token_embedding(y)\n",
        "        y = self.positional_embedding(y)\n",
        "\n",
        "        output_encoder = self.encoder(x, src_mask)\n",
        "        return self.to_logits(self.decoder(output_encoder, src_mask, y, trg_mask)).transpose(1,2)\n",
        "    \n",
        "    def configure_optimizers(self):# learning rate = 1x10^-4; beta1 =0.9; beta2 = 0.995 dal paper\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.995))\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #aggiungi gradient clipping nel Trainer\n",
        "        x, y = batch\n",
        "        y_pred = self(x, y)\n",
        "        loss = F.cross_entropy(y_pred, y, ignore_index = self.pad_id)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.inference(x) #controlla che sia [batch_size, vocabulary_size, question_len]\n",
        "        self.accuracy.update(y_pred, y)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.log('accuracy_epoch', self.accuracy.compute())\n",
        "        self.accuracy.reset()\n"
      ],
      "metadata": {
        "id": "qiPC8tqNY8wN"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOTA"
      ],
      "metadata": {
        "id": "sEka3B3Eq69g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp_transformer = Transformer(embedding_dim = 256, num_heads = 8, batch_size = 32, hidden_size = 512, dropout = 0.2, vocabulary_size = 58, bos_id = 0, pad_id = 3, num_blocks_encoder = 6, num_blocks_decoder = 6, tp_attention = True)\n",
        "trainer = Trainer(accelerator='auto', devices=1, gradient_clip_val = 0.1, max_epochs = 1)\n",
        "math_dm = Mathematics_DataModule(['./mathematics_dataset-v1.0/train-easy/algebra__linear_1d.txt'], batch_size = 32)\n",
        "trainer.fit(tp_transformer, datamodule = math_dm)\n",
        "#aggiungere folder per il log /content/drive/MyDrive/Deep_Learning_Project/lightning_logs\n"
      ],
      "metadata": {
        "id": "Uf3QJeF-q-AW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "2646c60c45914877900b986114afd422",
            "1f78aa206abd45f78e656e04afcf68a7",
            "a60ad952986f4a84aeff3b67cbb32b18",
            "ec41302ef3104f9196a31970b43989bb",
            "499734b415fd4fd8854e216be9e700ee",
            "cdbc0c96e46d4ae7bbca96ef5827b36f",
            "5e001073360d4b9293c38ab48ba4ccd1",
            "bd91fde42bbb433cbf48110ef2342cf3",
            "fcbfa75e054a412f988ac462da699c5a",
            "676c772548554b7ca2cb611917340758",
            "ac876b196f3d443f81d364c2598b25d0",
            "df57e6630ae4425aae228aba0865e583",
            "74c3c41f003e4836943a48aac8931852",
            "555fb76b98384709a048f8375615253d",
            "dae36f5062964b758374abc33d6f4fbe",
            "debcd5fb2fe94d9db9e035427a02dd17",
            "c7639369125e4856adf918cfe7265749",
            "a044b238d1be4f578116a360d59d96c0",
            "34117cb9bf1643e48144def77da11015",
            "e277f34cc6e440a3aa4af55bdde40db4",
            "25ee5226ba0c42159b131242709de363",
            "1b5aea2e659e44bb85fdb0860bf4187d"
          ]
        },
        "outputId": "b6c3c4ca-8141-48d2-c23c-095113600690"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                 | Type               | Params\n",
            "------------------------------------------------------------\n",
            "0 | token_embedding      | Embedding          | 14.8 K\n",
            "1 | positional_embedding | PositionalEncoding | 0     \n",
            "2 | encoder              | TransformerEncoder | 3.6 M \n",
            "3 | decoder              | TransformerDecoder | 5.5 M \n",
            "4 | to_logits            | Linear             | 14.9 K\n",
            "5 | accuracy             | MulticlassAccuracy | 0     \n",
            "------------------------------------------------------------\n",
            "9.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "9.1 M     Total params\n",
            "36.485    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2646c60c45914877900b986114afd422"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df57e6630ae4425aae228aba0865e583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_checkpoint(\"model_classic_transformer.ckpt\")"
      ],
      "metadata": {
        "id": "vzfHUGm_xyH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "da fare:\n",
        "-  drop_last = True nei dataloader perché altrimenti abbiamo sicuramente degli errori\n",
        "   (possibile alternativa: nella mha ricavare la batch size dalla prima dimensione dell'input \n",
        "    per evitare che dia errori di questo tipo durante l'inference con un solo input)\n",
        "\n",
        "-  callbacks=[TQDMProgressBar(refresh_rate=20)] da aggiungere come parametro al trainer\n",
        "\n",
        "-  aggiungere TensorBoardLogger\n",
        "    var = TensorBoardLogger(path, name=modello (tp o vanilla), log_graph=True)\n",
        "   (https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.TensorBoardLogger.html)\n",
        "\n",
        "-  utilizzare stage (parametro di setup) per caricare anche un solo dataset se stage = \"train\" ad esempio \n",
        "   (https://colab.research.google.com/drive/1oJrA-Q-neOl1fCQJhIWR_GmxpYaG-cFx?authuser=1#scrollTo=JM57yq7bJS0E)\n",
        "\n",
        "-  aggiungere predict_step nel pl.LightningModule dove si chiama inference e relativo predict dataloader nel Lightning data module\n",
        "\n",
        "-  in inference ha senso creare la causal mask? Serviva nel training per \n",
        "    considerare solo i caratteri utili, in questo caso consideriamo già un \n",
        "     carattere per volta sulla base delle predizioni precedenti\n",
        "\n",
        "-  RNN fatte molto bene:\n",
        "    https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch \n",
        "    https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S0igWQqiZV5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NON-SOTA"
      ],
      "metadata": {
        "id": "0b4LEKhjtizS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mm-mxAgbtmPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}